{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports and path setup. RDS connection via env or placeholder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-12T19:01:17.103980Z",
          "start_time": "2024-11-12T19:01:16.487371Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set RDS_CONNECTION_STRING in env or in this cell for backtesting.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from datetime import date, datetime\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# Parent of jupyter_notebook = aws_lambda_architecture\n",
        "module_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
        "if module_dir not in sys.path:\n",
        "    sys.path.insert(0, module_dir)\n",
        "\n",
        "import polars as pl\n",
        "from shared.analytics_core.inputs import load_ohlcv_from_rds, load_ohlcv_by_timeframe\n",
        "from shared.analytics_core.backtester import Backtester, BacktestResult\n",
        "from shared.analytics_core.strategies.library.momentum_strategies import GoldenCrossStrategy, RSIMomentumStrategy, MACDCrossoverStrategy\n",
        "from shared.analytics_core.strategies.library.breakout_strategies import BollingerBreakoutStrategy, ATRBreakoutStrategy\n",
        "from shared.analytics_core.strategies.library.vegas_channel_strategy import VegasChannelStrategy\n",
        "from shared.analytics_core.executor import MultiTimeframeExecutor\n",
        "from shared.analytics_core.indicators.technicals import calculate_all_indicators\n",
        "\n",
        "RDS_CONNECTION_STRING = os.environ.get(\"RDS_CONNECTION_STRING\", \"\")\n",
        "if not RDS_CONNECTION_STRING:\n",
        "    print(\"Set RDS_CONNECTION_STRING in env or in this cell for backtesting.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipped: RDS_CONNECTION_STRING not set.\n"
          ]
        }
      ],
      "source": [
        "# Load OHLCV for one symbol from RDS (for backtesting). Set RDS_CONNECTION_STRING first.\n",
        "SYMBOL = \"AAPL\"\n",
        "START_DATE = date(2023, 1, 1)\n",
        "END_DATE = date(2025, 1, 22)\n",
        "\n",
        "if RDS_CONNECTION_STRING:\n",
        "    df_ohlcv = load_ohlcv_from_rds(SYMBOL, RDS_CONNECTION_STRING, start_date=START_DATE, end_date=END_DATE, table_name=\"raw_ohlcv\")\n",
        "    print(f\"Loaded {df_ohlcv.height} rows for {SYMBOL}\")\n",
        "    df_ohlcv.head()\n",
        "else:\n",
        "    df_ohlcv = None\n",
        "    print(\"Skipped: RDS_CONNECTION_STRING not set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run strategy and backtest; store signals in memory (no MongoDB).\n",
        "if df_ohlcv is not None and not df_ohlcv.is_empty():\n",
        "    df_pl = df_ohlcv\n",
        "    if \"date\" not in df_pl.columns and \"timestamp\" in df_pl.columns:\n",
        "        df_pl = df_pl.rename({\"timestamp\": \"date\"})\n",
        "    df_pl = calculate_all_indicators(df_pl)\n",
        "    strategy = GoldenCrossStrategy()\n",
        "    df_with_signals = strategy.run(df_pl)\n",
        "    backtester = Backtester(initial_capital=10000.0)\n",
        "    result = backtester.run(strategy, df_with_signals, stop_loss_pct=0.05, take_profit_pct=0.10)\n",
        "    signals_in_memory = strategy.get_signals(df_with_signals)\n",
        "    print(f\"Backtest: {result.total_trades} trades, win_rate={result.win_rate:.2%}, return_pct={(result.final_capital/result.initial_capital - 1)*100:.2f}%\")\n",
        "    signals_in_memory\n",
        "else:\n",
        "    signals_in_memory = pl.DataFrame()\n",
        "    print(\"Skipped: no OHLCV data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         BBAR\n",
              "1          IEF\n",
              "2         ACIO\n",
              "3         EAOA\n",
              "4         RBLX\n",
              "         ...  \n",
              "15139    ZVZZT\n",
              "15140    ZTEST\n",
              "15141    ZJZZT\n",
              "15142     ZBZX\n",
              "15143    ZIEXT\n",
              "Name: ticker, Length: 15144, dtype: object"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from polygon import RESTClient\n",
        "import pandas as pd \n",
        "from src.utils.config import (\n",
        "    POLYGON_API_KEY\n",
        ")\n",
        "\n",
        "client = RESTClient(api_key=POLYGON_API_KEY)\n",
        "\n",
        "mkdata = pd.DataFrame(client.get_grouped_daily_aggs(\n",
        "    date=\"2025-03-03\",\n",
        "    include_otc=True\n",
        "))\n",
        "\n",
        "mkdata['ticker']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved 5185 common stocks\n"
          ]
        }
      ],
      "source": [
        "# Get all tickers - using list comprehension for better performance\n",
        "ticker_list = []\n",
        "market_type = \"stocks\"  # can be stocks, crypto, fx\n",
        "\n",
        "# Fetch all tickers without pagination since next_token parameter is not supported\n",
        "try:\n",
        "    tickers_response = client.list_tickers(\n",
        "        market=market_type,\n",
        "        active=True,\n",
        "        limit=1000  # max limit per request\n",
        "    )\n",
        "    \n",
        "    # Filter and extract only common stocks (CS) in one step\n",
        "    ticker_list.extend([{\n",
        "        'symbol': ticker.ticker,\n",
        "        'name': ticker.name,\n",
        "        'market': ticker.market,\n",
        "        'type': ticker.type,\n",
        "        'active': ticker.active,\n",
        "        'primary_exchange': ticker.primary_exchange,\n",
        "        'currency_name': ticker.currency_name,\n",
        "    } for ticker in tickers_response if ticker.type == 'CS'])\n",
        "    \n",
        "    # Create DataFrame directly from the filtered list\n",
        "    df = pd.DataFrame(ticker_list)\n",
        "    print(f\"Retrieved {len(ticker_list)} common stocks\")\n",
        "    df\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error fetching tickers: {e}\")\n",
        "    \n",
        "    # Fallback to get_tickers method if available\n",
        "    try:\n",
        "        tickers = client.get_tickers(market=market_type, active=True, limit=1000)\n",
        "        ticker_list = [{\n",
        "            'symbol': ticker.ticker,\n",
        "            'name': ticker.name,\n",
        "            'market': ticker.market,\n",
        "            'type': ticker.type,\n",
        "            'active': ticker.active,\n",
        "            'primary_exchange': ticker.primary_exchange,\n",
        "            'currency_name': ticker.currency_name,\n",
        "        } for ticker in tickers if ticker.type == 'CS']\n",
        "        df = pd.DataFrame(ticker_list)\n",
        "        print(f\"Retrieved {len(ticker_list)} common stocks using fallback method\")\n",
        "        df\n",
        "    except Exception as e2:\n",
        "        print(f\"Fallback method also failed: {e2}\")\n",
        "        df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11149 entries, 0 to 11148\n",
            "Data columns (total 6 columns):\n",
            " #   Column  Non-Null Count  Dtype                           \n",
            "---  ------  --------------  -----                           \n",
            " 0   t       11149 non-null  datetime64[ns, America/New_York]\n",
            " 1   o       11149 non-null  float64                         \n",
            " 2   h       11149 non-null  float64                         \n",
            " 3   l       11149 non-null  float64                         \n",
            " 4   c       11149 non-null  float64                         \n",
            " 5   v       11149 non-null  int64                           \n",
            "dtypes: datetime64[ns, America/New_York](1), float64(4), int64(1)\n",
            "memory usage: 522.7 KB\n"
          ]
        }
      ],
      "source": [
        "data_df = yf.Ticker('AAPL').history(\n",
        "    period='max',\n",
        "    interval='1d'\n",
        ").reset_index()\n",
        "data_df = data_df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "data_df.columns = ['t', 'o', 'h', 'l', 'c', 'v']\n",
        "data_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-04-18     Good Friday     (NYSE)\n",
            "2025-04-18     Good Friday     (NASDAQ)\n",
            "2025-05-26     Memorial Day    (NASDAQ)\n",
            "2025-05-26     Memorial Day    (NYSE)\n",
            "2025-06-19     Juneteenth      (NASDAQ)\n",
            "2025-06-19     Juneteenth      (NYSE)\n",
            "2025-07-03     Independence Day (NYSE)\n",
            "2025-07-03     Independence Day (NASDAQ)\n",
            "2025-07-04     Independence Day (NASDAQ)\n",
            "2025-07-04     Independence Day (NYSE)\n",
            "2025-09-01     Labor Day       (NASDAQ)\n",
            "2025-09-01     Labor Day       (NYSE)\n",
            "2025-11-27     Thanksgiving    (NASDAQ)\n",
            "2025-11-27     Thanksgiving    (NYSE)\n",
            "2025-11-28     Thanksgiving    (NYSE)\n",
            "2025-11-28     Thanksgiving    (NASDAQ)\n",
            "2025-12-24     Christmas       (NYSE)\n",
            "2025-12-24     Christmas       (NASDAQ)\n",
            "2025-12-25     Christmas       (NASDAQ)\n",
            "2025-12-25     Christmas       (NYSE)\n",
            "2026-01-01     New Years Day   (NASDAQ)\n",
            "2026-01-01     New Years Day   (NYSE)\n",
            "2026-01-19     Martin Luther King, Jr. Day (NASDAQ)\n",
            "2026-01-19     Martin Luther King, Jr. Day (NYSE)\n",
            "2026-02-16     Washington's Birthday (NASDAQ)\n",
            "2026-02-16     Washington's Birthday (NYSE)\n"
          ]
        }
      ],
      "source": [
        "from polygon import RESTClient\n",
        "from polygon.rest.models import (\n",
        "    MarketHoliday,\n",
        ")\n",
        "\n",
        "holidays = client.get_market_holidays()\n",
        "\n",
        "for holiday in holidays:\n",
        "    # verify this is an exchange\n",
        "    if isinstance(holiday, MarketHoliday):\n",
        "        print(\"{:<15}{:<15} ({})\".format(holiday.date, holiday.name, holiday.exchange))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2024-03-04 05:00:00</th>\n",
              "      <td>176.15</td>\n",
              "      <td>176.9000</td>\n",
              "      <td>173.790</td>\n",
              "      <td>175.10</td>\n",
              "      <td>81505451.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-03-05 05:00:00</th>\n",
              "      <td>170.76</td>\n",
              "      <td>172.0400</td>\n",
              "      <td>169.620</td>\n",
              "      <td>170.12</td>\n",
              "      <td>94702355.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-03-06 05:00:00</th>\n",
              "      <td>171.06</td>\n",
              "      <td>171.2400</td>\n",
              "      <td>168.680</td>\n",
              "      <td>169.12</td>\n",
              "      <td>68568907.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-03-07 05:00:00</th>\n",
              "      <td>169.15</td>\n",
              "      <td>170.7300</td>\n",
              "      <td>168.490</td>\n",
              "      <td>169.00</td>\n",
              "      <td>71763761.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-03-08 05:00:00</th>\n",
              "      <td>169.00</td>\n",
              "      <td>173.7000</td>\n",
              "      <td>168.940</td>\n",
              "      <td>170.73</td>\n",
              "      <td>76267041.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-02-25 05:00:00</th>\n",
              "      <td>248.00</td>\n",
              "      <td>250.0000</td>\n",
              "      <td>244.910</td>\n",
              "      <td>247.04</td>\n",
              "      <td>44242095.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-02-26 05:00:00</th>\n",
              "      <td>244.33</td>\n",
              "      <td>244.9800</td>\n",
              "      <td>239.130</td>\n",
              "      <td>240.36</td>\n",
              "      <td>40678483.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-02-27 05:00:00</th>\n",
              "      <td>239.41</td>\n",
              "      <td>242.4600</td>\n",
              "      <td>237.060</td>\n",
              "      <td>237.30</td>\n",
              "      <td>38720764.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-02-28 05:00:00</th>\n",
              "      <td>236.95</td>\n",
              "      <td>242.0900</td>\n",
              "      <td>230.200</td>\n",
              "      <td>241.84</td>\n",
              "      <td>52500888.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-03-03 05:00:00</th>\n",
              "      <td>241.79</td>\n",
              "      <td>244.0272</td>\n",
              "      <td>236.112</td>\n",
              "      <td>238.03</td>\n",
              "      <td>43107096.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       open      high      low   close      volume\n",
              "timestamp                                                         \n",
              "2024-03-04 05:00:00  176.15  176.9000  173.790  175.10  81505451.0\n",
              "2024-03-05 05:00:00  170.76  172.0400  169.620  170.12  94702355.0\n",
              "2024-03-06 05:00:00  171.06  171.2400  168.680  169.12  68568907.0\n",
              "2024-03-07 05:00:00  169.15  170.7300  168.490  169.00  71763761.0\n",
              "2024-03-08 05:00:00  169.00  173.7000  168.940  170.73  76267041.0\n",
              "...                     ...       ...      ...     ...         ...\n",
              "2025-02-25 05:00:00  248.00  250.0000  244.910  247.04  44242095.0\n",
              "2025-02-26 05:00:00  244.33  244.9800  239.130  240.36  40678483.0\n",
              "2025-02-27 05:00:00  239.41  242.4600  237.060  237.30  38720764.0\n",
              "2025-02-28 05:00:00  236.95  242.0900  230.200  241.84  52500888.0\n",
              "2025-03-03 05:00:00  241.79  244.0272  236.112  238.03  43107096.0\n",
              "\n",
              "[250 rows x 5 columns]"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fetch 1 day apple stock price from polygon\n",
        "apple_df = client.get_aggs(\n",
        "    ticker='AAPL',\n",
        "    multiplier=1,\n",
        "    timespan='day',\n",
        "    from_='2024-03-03',\n",
        "    to='2025-03-03'\n",
        ")\n",
        "apple_df = pd.DataFrame([{\n",
        "    'timestamp': agg.timestamp,\n",
        "    'open': agg.open,\n",
        "    'high': agg.high,\n",
        "    'low': agg.low,\n",
        "    'close': agg.close,\n",
        "    'volume': agg.volume \n",
        "} for agg in apple_df])\n",
        "\n",
        "# Convert to dask dataframe\n",
        "apple_df = apple_df.set_index(\n",
        "    pd.to_datetime(apple_df['timestamp'], unit='ms')\n",
        ").drop('timestamp', axis=1)\n",
        "\n",
        "apple_df = dd.from_pandas(apple_df, npartitions=10)\n",
        "apple_df.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Length mismatch: Expected 0 rows, received array of length 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/pv/rbjx7k1j5cs6jxs5pskztclh0000gn/T/ipykernel_20367/1658685581.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert timestamp to datetime and set as index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m apple_df = apple_df.set_index(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapple_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ms'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Projects/condvest/stock_datapipeline/.condvest_dp/lib/python3.13/site-packages/dask/dataframe/dask_expr/_collection.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, other, drop, sorted, npartitions, divisions, sort, shuffle_method, upsample, partition_size, append, **options)\u001b[0m\n\u001b[1;32m   3502\u001b[0m             return new_collection(\n\u001b[1;32m   3503\u001b[0m                 \u001b[0mSetIndexBlockwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3506\u001b[0;31m         return new_collection(\n\u001b[0m\u001b[1;32m   3507\u001b[0m             SetIndex(\n\u001b[1;32m   3508\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m                 \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Projects/condvest/stock_datapipeline/.condvest_dp/lib/python3.13/site-packages/dask/_collections.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(expr)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnew_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"\"\"Create new collection from an expr\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m  \u001b[0;31m# Ensure backend is imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_collection_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NOT_FOUND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_NOT_FOUND\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Projects/condvest/stock_datapipeline/.condvest_dp/lib/python3.13/site-packages/dask/dataframe/dask_expr/_shuffle.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_other\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_other\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_other\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/Documents/Projects/condvest/stock_datapipeline/.condvest_dp/lib/python3.13/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   6169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6171\u001b[0m                 \u001b[0;31m# check newest element against length of calling frame, since\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6172\u001b[0m                 \u001b[0;31m# ensure_index_from_sequences would not raise for append=False.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6173\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   6174\u001b[0m                     \u001b[0;34mf\"\u001b[0m\u001b[0;34mLength mismatch: Expected \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m rows, \u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6175\u001b[0m                     \u001b[0;34mf\"\u001b[0m\u001b[0;34mreceived array of length \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6176\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected 0 rows, received array of length 1"
          ]
        }
      ],
      "source": [
        "# Convert timestamp to datetime and set as index\n",
        "apple_df = apple_df.set_index(\n",
        "    dd.to_datetime(apple_df['timestamp'], unit='ms')\n",
        ").drop('timestamp', axis=1)\n",
        "\n",
        "# Resample to 3 business days\n",
        "apple_df = apple_df.resample('3B').agg({\n",
        "    'open': 'first',\n",
        "    'high': 'max',\n",
        "    'low': 'min', \n",
        "    'close': 'last',\n",
        "    'volume': 'sum',\n",
        "    'vwap': 'mean',\n",
        "    'transactions': 'sum'\n",
        "}).compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>vwap</th>\n",
              "      <th>transactions</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2024-03-04</th>\n",
              "      <td>176.15</td>\n",
              "      <td>176.9000</td>\n",
              "      <td>173.790</td>\n",
              "      <td>175.10</td>\n",
              "      <td>81505451.0</td>\n",
              "      <td>174.8938</td>\n",
              "      <td>1167166.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-03-05</th>\n",
              "      <td>170.76</td>\n",
              "      <td>172.0400</td>\n",
              "      <td>169.620</td>\n",
              "      <td>170.12</td>\n",
              "      <td>94702355.0</td>\n",
              "      <td>170.3234</td>\n",
              "      <td>1108820.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-03-06</th>\n",
              "      <td>171.06</td>\n",
              "      <td>171.2400</td>\n",
              "      <td>168.680</td>\n",
              "      <td>169.12</td>\n",
              "      <td>68568907.0</td>\n",
              "      <td>169.5506</td>\n",
              "      <td>896297.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-03-07</th>\n",
              "      <td>169.15</td>\n",
              "      <td>170.7300</td>\n",
              "      <td>168.490</td>\n",
              "      <td>169.00</td>\n",
              "      <td>71763761.0</td>\n",
              "      <td>169.3619</td>\n",
              "      <td>825405.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-03-08</th>\n",
              "      <td>169.00</td>\n",
              "      <td>173.7000</td>\n",
              "      <td>168.940</td>\n",
              "      <td>170.73</td>\n",
              "      <td>76267041.0</td>\n",
              "      <td>171.5322</td>\n",
              "      <td>925213.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-02-25</th>\n",
              "      <td>248.00</td>\n",
              "      <td>250.0000</td>\n",
              "      <td>244.910</td>\n",
              "      <td>247.04</td>\n",
              "      <td>44242095.0</td>\n",
              "      <td>247.5126</td>\n",
              "      <td>573942.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-02-26</th>\n",
              "      <td>244.33</td>\n",
              "      <td>244.9800</td>\n",
              "      <td>239.130</td>\n",
              "      <td>240.36</td>\n",
              "      <td>40678483.0</td>\n",
              "      <td>241.7765</td>\n",
              "      <td>557126.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-02-27</th>\n",
              "      <td>239.41</td>\n",
              "      <td>242.4600</td>\n",
              "      <td>237.060</td>\n",
              "      <td>237.30</td>\n",
              "      <td>38720764.0</td>\n",
              "      <td>239.6332</td>\n",
              "      <td>503371.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-02-28</th>\n",
              "      <td>236.95</td>\n",
              "      <td>242.0900</td>\n",
              "      <td>230.200</td>\n",
              "      <td>241.84</td>\n",
              "      <td>52500888.0</td>\n",
              "      <td>239.3193</td>\n",
              "      <td>572364.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-03-03</th>\n",
              "      <td>241.79</td>\n",
              "      <td>244.0272</td>\n",
              "      <td>236.112</td>\n",
              "      <td>238.03</td>\n",
              "      <td>43107096.0</td>\n",
              "      <td>240.0800</td>\n",
              "      <td>556946.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>261 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              open      high      low   close      volume      vwap  \\\n",
              "timestamp                                                             \n",
              "2024-03-04  176.15  176.9000  173.790  175.10  81505451.0  174.8938   \n",
              "2024-03-05  170.76  172.0400  169.620  170.12  94702355.0  170.3234   \n",
              "2024-03-06  171.06  171.2400  168.680  169.12  68568907.0  169.5506   \n",
              "2024-03-07  169.15  170.7300  168.490  169.00  71763761.0  169.3619   \n",
              "2024-03-08  169.00  173.7000  168.940  170.73  76267041.0  171.5322   \n",
              "...            ...       ...      ...     ...         ...       ...   \n",
              "2025-02-25  248.00  250.0000  244.910  247.04  44242095.0  247.5126   \n",
              "2025-02-26  244.33  244.9800  239.130  240.36  40678483.0  241.7765   \n",
              "2025-02-27  239.41  242.4600  237.060  237.30  38720764.0  239.6332   \n",
              "2025-02-28  236.95  242.0900  230.200  241.84  52500888.0  239.3193   \n",
              "2025-03-03  241.79  244.0272  236.112  238.03  43107096.0  240.0800   \n",
              "\n",
              "            transactions  \n",
              "timestamp                 \n",
              "2024-03-04     1167166.0  \n",
              "2024-03-05     1108820.0  \n",
              "2024-03-06      896297.0  \n",
              "2024-03-07      825405.0  \n",
              "2024-03-08      925213.0  \n",
              "...                  ...  \n",
              "2025-02-25      573942.0  \n",
              "2025-02-26      557126.0  \n",
              "2025-02-27      503371.0  \n",
              "2025-02-28      572364.0  \n",
              "2025-03-03      556946.0  \n",
              "\n",
              "[261 rows x 7 columns]"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "apple_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alerts Labelling EDA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alerts Labelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AddAlert:\n",
        "    def __init__(self, df_dict, interval, symbol):\n",
        "\n",
        "        self.df_dict = [entry for entry in df_dict if entry['interval'] == interval]\n",
        "        self.window = 30  # Number of days to compare the stock price\n",
        "        self.interval = interval\n",
        "        self.symbol = symbol\n",
        "        self.dict = {}\n",
        "        self.rolling_window = 50\n",
        "\n",
        "    def velocity_accel_decel_alert(self, base_window=28):\n",
        "        # Initialize the list to keep track of the last `obs_window` velocity statuses\n",
        "        previous_velocity_status = None \n",
        "        alert_duration = 0\n",
        "        window_dict = {\n",
        "            1: base_window,      \n",
        "            3: base_window - 8,   \n",
        "            5: base_window - 8,   \n",
        "            8: base_window - 14,   \n",
        "            13: base_window - 14   \n",
        "        }\n",
        "\n",
        "        # Loop through the stock data to calculate velocity acceleration and deceleration alerts\n",
        "        for i in range(len(self.df_dict)):\n",
        "            \n",
        "            entry = self.df_dict[i]  # Current row (entry) of stock data\n",
        "            \n",
        "            # Ensure 'alerts' dictionary exists in the entry\n",
        "            if 'alerts' not in entry:\n",
        "                entry['alerts'] = {}\n",
        "                \n",
        "            interval = entry['interval']\n",
        "            obs_window = window_dict.get(interval, base_window//4)\n",
        "            \n",
        "            # Skip the first `obs_window` days (cannot determine acceleration before that)\n",
        "            if i < obs_window:\n",
        "                continue\n",
        "    \n",
        "            # Record velocity statuses for the last `obs_window` days\n",
        "            previous_window = self.df_dict[i-obs_window:i]  # Slice the last `obs_window` data points\n",
        "    \n",
        "            # Initialize current velocity status\n",
        "            current_velocity_status = None\n",
        "\n",
        "            # Calculate velocity counts in the observation window\n",
        "            count_velocity_loss = sum(\n",
        "                1 for row in previous_window\n",
        "                if 'velocity_alert' in row['alerts'] and\n",
        "                row['alerts']['velocity_alert']['alert_type'] in ['velocity_loss', 'velocity_weak',\n",
        "                                                                'velocity_negotiating']\n",
        "            )\n",
        "    \n",
        "            count_velocity_maintained = sum(\n",
        "                1 for row in previous_window\n",
        "                if 'velocity_alert' in row['alerts'] and\n",
        "                row['alerts']['velocity_alert']['alert_type'] == 'velocity_maintained'\n",
        "            )\n",
        "            # Check for deceleration in the last 10 days (skip if any decelerated alert is found)\n",
        "            short_term_max = max(entry['8ema'], entry['13ema'])\n",
        "            lng_term_max = max(entry['169ema'], entry['144ema'])\n",
        "            \n",
        "            short_term_min = min(entry['8ema'], entry['13ema'])\n",
        "            lng_term_min = min(entry['169ema'], entry['144ema'])\n",
        "            \n",
        "            if np.isnan(entry['144ema']):\n",
        "                lng_term_max = entry['13ema']\n",
        "                lng_term_min = entry['13ema']\n",
        "                short_term_max = entry['8ema']\n",
        "                short_term_min = entry['8ema']\n",
        "    \n",
        "            # Check for velocity acceleration (EMA8 crosses above EMA13) and use the observation window counts\n",
        "            if lng_term_max <= short_term_max < entry['open'] < entry['close']: \n",
        "                # Trigger acceleration alert if the count of 'velocity_weak' exceeds 'velocity_maintained'\n",
        "                if count_velocity_loss > count_velocity_maintained:\n",
        "                    alert_duration += 1\n",
        "                    if'accelerated' not in previous_window:\n",
        "                        if 'velocity_accelerated' not in entry['alerts']:\n",
        "                            current_velocity_status = 'accelerated'\n",
        "\n",
        "        # Check for velocity deceleration (EMA8 crosses below EMA13)\n",
        "            elif entry['close'] < short_term_min <= lng_term_min:\n",
        "                # velocity_maintained' exceeds 'velocity_weak'\n",
        "                if count_velocity_maintained < count_velocity_loss:\n",
        "                    alert_duration += 1\n",
        "                    if 'decelerated' not in previous_window:\n",
        "                        if 'velocity_decelerated' not in entry['alerts']:\n",
        "                            current_velocity_status = 'decelerated'\n",
        "                            \n",
        "            # Add the velocity acceleration or deceleration alert to the entry\n",
        "            allow_new_alert = (current_velocity_status != previous_velocity_status \\\n",
        "                or alert_duration >= 30)\n",
        "                \n",
        "            if current_velocity_status and allow_new_alert:\n",
        "                \n",
        "                entry['alerts']['momentum_alert'] = {\n",
        "                    \"date\": entry['date'],\n",
        "                    \"alert_type\": current_velocity_status,\n",
        "                    \"details\": f\"Velocity status changed: {current_velocity_status}\"\n",
        "                }\n",
        "                # Update previous_velocity_status to the current one\n",
        "                previous_velocity_status = current_velocity_status\n",
        "                alert_duration = 0\n",
        "            elif alert_duration >= 30:\n",
        "                alert_duration = 0\n",
        "                previous_velocity_status = None\n",
        "\n",
        "    def velocity_alert_dict(self, window=3):\n",
        "\n",
        "        # Loop through the data to calculate the velocity alerts\n",
        "        for i in range(len(self.df_dict)):\n",
        "            # Skip rows before the window starts to avoid indexing errors\n",
        "            if i < window:\n",
        "                continue\n",
        "\n",
        "            # Initialize variables for the current entry\n",
        "            entry = self.df_dict[i]\n",
        "\n",
        "            # Ensure 'alerts' dictionary exists in the entry\n",
        "            if 'alerts' not in entry:\n",
        "                entry['alerts'] = {}\n",
        "\n",
        "            # Condition where closing price is above both EMAs\n",
        "            above_13ema = entry['close'] > max(entry['13ema'], entry['8ema'])\n",
        "            above_169ema = entry['close'] > max(entry['169ema'], entry['144ema'])\n",
        "\n",
        "            # Condition where closing price is below both EMAs\n",
        "            below_13ema = entry['close'] < entry['13ema']\n",
        "            below_169ema = entry['close'] < entry['169ema']\n",
        "\n",
        "            # Define long-term and short-term bounds\n",
        "            lng_term_max = max(entry['144ema'], entry['169ema'])\n",
        "            lng_term_min = min(entry['144ema'], entry['169ema'])\n",
        "\n",
        "            short_term_max = max(entry['13ema'], entry['8ema'])\n",
        "            short_term_min = min(entry['13ema'], entry['8ema'])\n",
        "\n",
        "            # Handle the case when 144ema is missing\n",
        "            if np.isnan(entry['144ema']):\n",
        "                above_169ema = above_13ema\n",
        "                below_169ema = below_13ema\n",
        "                lng_term_max = lng_term_min = entry['13ema']\n",
        "                short_term_max = short_term_min = entry['8ema']\n",
        "\n",
        "            # Condition where closing price is between 13EMA and 169EMA\n",
        "            between_13_169ema = entry['13ema'] >= entry['close'] >= entry['169ema']\n",
        "\n",
        "            # Condition where ema8 and ema13 is above ema144 and ema169\n",
        "            in_up_trend = short_term_min > lng_term_max\n",
        "            in_down_trend = short_term_max < lng_term_min\n",
        "\n",
        "            # Check for the 'velocity_maintained' alert (above both EMAs)\n",
        "            if above_13ema and above_169ema and in_up_trend:\n",
        "                current_alert_type = 'velocity_maintained'\n",
        "\n",
        "            # Check for the 'velocity_weak' alert (sandwiched between 13EMA and 169EMA)\n",
        "            elif between_13_169ema and below_13ema:\n",
        "                current_alert_type = 'velocity_weak'\n",
        "\n",
        "            # Check for the 'velocity_loss' alert (below either 13EMA or 169EMA)\n",
        "            elif below_169ema and below_13ema:\n",
        "                current_alert_type = 'velocity_loss'\n",
        "\n",
        "            else:\n",
        "                current_alert_type = 'velocity_negotiating'\n",
        "\n",
        "            # Add the alert only if it's different from the previous alert\n",
        "            if current_alert_type:\n",
        "                entry['alerts']['velocity_alert'] = {\n",
        "                    \"date\": entry['date'],\n",
        "                    \"alert_type\": current_alert_type,\n",
        "                    \"details\": f\"Velocity alert triggered: {current_alert_type}\"}\n",
        "\n",
        "    def add_169ema_touch_alert(self):\n",
        "\n",
        "        touch_sup_count_window = deque(maxlen=self.rolling_window)\n",
        "        touch_res_count_window = deque(maxlen=self.rolling_window)\n",
        "        recent_touch_count = 0\n",
        "        tolerance_dict = {interval: tolerance for interval, tolerance in\n",
        "                        zip([1, 3, 5, 8, 13], [0.002, 0.02, 0.05, 0.07, 0.1])}\n",
        "\n",
        "        for i, entry in enumerate(self.df_dict):\n",
        "\n",
        "            # Define the upper and lower bounds for the tolerance range\n",
        "            alert_type = 'neutral'\n",
        "            lower_bound = 0\n",
        "            upper_bound = 0\n",
        "            interval = entry['interval']\n",
        "            # Define Upper and Lower Bound based on available EMAs\n",
        "            if not np.isnan(entry.get('144ema', np.nan)) and not np.isnan(entry.get('169ema', np.nan)):\n",
        "                lower_bound = min(entry['144ema'], entry['169ema']) * (1 - tolerance_dict[interval])\n",
        "                upper_bound = max(entry['144ema'], entry['169ema']) * (1 + tolerance_dict[interval])\n",
        "            elif not np.isnan(entry.get('144ema', np.nan)):\n",
        "                lower_bound = entry['144ema'] * (1 - tolerance_dict[interval])\n",
        "                upper_bound = entry['144ema'] * (1 + tolerance_dict[interval])\n",
        "            elif not np.isnan(entry.get('13ema', np.nan)) and not np.isnan(entry.get('8ema', np.nan)):\n",
        "                lower_bound = min(entry['13ema'], entry['8ema']) * (1 - tolerance_dict[interval])\n",
        "                upper_bound = max(entry['13ema'], entry['8ema']) * (1 + tolerance_dict[interval])\n",
        "            elif not np.isnan(entry.get('8ema', np.nan)):\n",
        "                lower_bound = entry['8ema'] * (1 - tolerance_dict[interval])\n",
        "                upper_bound = entry['8ema'] * (1 + tolerance_dict[interval])\n",
        "\n",
        "            # Ensure bounds are defined before checking closing price range\n",
        "            if lower_bound is not None and upper_bound is not None:\n",
        "                if (lower_bound <= entry['low'] <= upper_bound) or \\\n",
        "                        (lower_bound <= entry.get('13ema', 0) <= upper_bound) or \\\n",
        "                        (lower_bound <= entry.get('8ema', 0) <= upper_bound):\n",
        "\n",
        "                    # Define Tunnel Max and Min safely\n",
        "                    lng_term_tunnel_max = max(entry.get('169ema', 0), entry.get('144ema', 0))\n",
        "                    lng_term_tunnel_min = min(entry.get('169ema', 0), entry.get('144ema', 0))\n",
        "                    short_term_tunnel_max = max(entry.get('13ema', 0), entry.get('8ema', 0))\n",
        "                    short_term_tunnel_min = min(entry.get('13ema', 0), entry.get('8ema', 0))\n",
        "                    candle_min = min(entry['close'], entry['open'])\n",
        "                    candle_max = max(entry['close'], entry['open'])\n",
        "\n",
        "                    if np.isnan(entry['169ema']) and np.isnan(entry['144ema']):\n",
        "                        lng_term_tunnel_max = short_term_tunnel_max\n",
        "                        short_term_tunnel_max = candle_max\n",
        "                    if np.isnan(entry['169ema']) and np.isnan(entry['144ema']):\n",
        "                        lng_term_tunnel_min = short_term_tunnel_min\n",
        "                        short_term_tunnel_min = candle_min\n",
        "\n",
        "                    # Determine if the touch is from above (support) or below (resistance) using 13EMA > 169EMA\n",
        "                    if short_term_tunnel_min > lng_term_tunnel_max \\\n",
        "                            and (candle_min > lng_term_tunnel_min):\n",
        "                        alert_type = 'support'  # Touched from above\n",
        "\n",
        "                    elif short_term_tunnel_max < lng_term_tunnel_max \\\n",
        "                            and (entry['close'] < lng_term_tunnel_max):\n",
        "                        alert_type = 'resistance'  # Touched from below\n",
        "\n",
        "                    # Check for similar alert in the last 3 entries to avoid duplication\n",
        "                    if i >= 3 and any(\n",
        "                            'alerts' in self.df_dict[j] and\n",
        "                            '169ema_touched' in self.df_dict[j]['alerts'] and\n",
        "                            self.df_dict[j]['alerts']['169ema_touched']['type'] == alert_type\n",
        "                            for j in range(i - 3, i)\n",
        "                    ):\n",
        "                        continue  # Skip adding alert if similar alert exists\n",
        "\n",
        "                    # Update touch count window and add to recent touch count\n",
        "                    if alert_type == 'support':\n",
        "                        touch_sup_count_window.append(1)\n",
        "                        recent_touch_count = sum(touch_sup_count_window)\n",
        "                    elif alert_type == 'resistance':\n",
        "                        touch_res_count_window.append(1)\n",
        "                        recent_touch_count = sum(touch_res_count_window)\n",
        "\n",
        "                    # Add alert to entry\n",
        "                    if 'alerts' not in entry:\n",
        "                        entry['alerts'] = {}\n",
        "\n",
        "                    # Record the touch alert with the updated count\n",
        "                    entry['alerts']['169ema_touched'] = {\n",
        "                        \"date\": entry['date'],\n",
        "                        \"type\": alert_type,  # 'support' or 'resistance'\n",
        "                        \"count\": recent_touch_count,  # count of touching support/resistance\n",
        "                        \"169ema\": entry['169ema'],\n",
        "                        \"details\": f\"Close price touched EMA169 from {alert_type} within {tolerance_dict[interval] * 100}% tolerance\"\n",
        "                    }\n",
        "                else:\n",
        "                    # No touch: add zeros to maintain window length\n",
        "                    touch_sup_count_window.append(0)\n",
        "                    touch_res_count_window.append(0)\n",
        "\n",
        "    # Function to filter only the required keys (symbol, interval, alerts)\n",
        "\n",
        "    def filter_data(self):\n",
        "        filtered_data = {}\n",
        "        filtered_data[self.symbol] = [\n",
        "            {\"date\": record['date'],\n",
        "            'interval': record['interval'],  # Keep interval\n",
        "            'alerts': record['alerts']  # Keep alerts\n",
        "            }\n",
        "            for record in self.df_dict  # Iterate over the list directly\n",
        "        ]\n",
        "\n",
        "        return filtered_data\n",
        "\n",
        "    def apply(self):\n",
        "        self.add_169ema_touch_alert()\n",
        "        self.velocity_alert_dict()\n",
        "        self.velocity_accel_decel_alert()\n",
        "        self.filter_data()\n",
        "\n",
        "        return self.df_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alert Labelling (Vectorized Under Development)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class AddAlert:\n",
        "#     def __init__(self, df_dict, interval, symbol):\n",
        "#         # Convert list of dictionaries to DataFrame\n",
        "#         self.df = pd.DataFrame(df_dict)\n",
        "#         self.df = self.df[self.df['interval'] == interval].copy()\n",
        "#         self.window = 30\n",
        "#         self.interval = interval \n",
        "#         self.symbol = symbol\n",
        "#         self.dict = {}\n",
        "#         self.rolling_window = 50\n",
        "\n",
        "#     def velocity_accel_decel_alert(self, base_window=28):\n",
        "#         window_dict = {\n",
        "#             1: base_window,\n",
        "#             3: base_window - 8,\n",
        "#             5: base_window - 8, \n",
        "#             8: base_window - 14,\n",
        "#             13: base_window - 14\n",
        "#         }\n",
        "        \n",
        "#         # Initialize alerts column if not exists\n",
        "#         if 'alerts' not in self.df.columns:\n",
        "#             self.df['alerts'] = [{} for _ in range(len(self.df))]\n",
        "\n",
        "#         obs_window = window_dict.get(self.interval, base_window//4)\n",
        "        \n",
        "#         # Calculate velocity counts using rolling window\n",
        "#         velocity_loss_mask = self.df['alerts'].apply(lambda x: 'velocity_alert' in x and \n",
        "#                                                 x.get('velocity_alert',{}).get('alert_type') in \n",
        "#                                                 ['velocity_loss','velocity_weak','velocity_negotiating'])\n",
        "        \n",
        "#         velocity_maintained_mask = self.df['alerts'].apply(lambda x: 'velocity_alert' in x and\n",
        "#                                                         x.get('velocity_alert',{}).get('alert_type') == 'velocity_maintained')\n",
        "        \n",
        "#         count_velocity_loss = velocity_loss_mask.rolling(obs_window).sum()\n",
        "#         count_velocity_maintained = velocity_maintained_mask.rolling(obs_window).sum()\n",
        "\n",
        "#         # Calculate conditions vectorized\n",
        "#         short_term_max = self.df[['8ema','13ema']].max(axis=1)\n",
        "#         lng_term_max = self.df[['169ema','144ema']].max(axis=1)\n",
        "#         short_term_min = self.df[['8ema','13ema']].min(axis=1)\n",
        "#         lng_term_min = self.df[['169ema','144ema']].min(axis=1)\n",
        "\n",
        "#         # Handle NaN values\n",
        "#         mask_nan = self.df['144ema'].isna()\n",
        "#         lng_term_max[mask_nan] = self.df['13ema'][mask_nan]\n",
        "#         lng_term_min[mask_nan] = self.df['13ema'][mask_nan] \n",
        "#         short_term_max[mask_nan] = self.df['8ema'][mask_nan]\n",
        "#         short_term_min[mask_nan] = self.df['8ema'][mask_nan]\n",
        "\n",
        "#         # Calculate acceleration/deceleration conditions\n",
        "#         accel_cond = (lng_term_max <= short_term_max) & (short_term_max < self.df['open']) & (self.df['open'] < self.df['close'])\n",
        "#         decel_cond = (self.df['close'] < short_term_min) & (short_term_min <= lng_term_min)\n",
        "\n",
        "#         # Apply alerts based on conditions\n",
        "#         for i in range(obs_window, len(self.df)):\n",
        "#             if accel_cond.iloc[i] and count_velocity_loss.iloc[i] > count_velocity_maintained.iloc[i]:\n",
        "#                 self.df.at[i,'alerts']['momentum_alert'] = {\n",
        "#                     \"date\": self.df.iloc[i]['date'],\n",
        "#                     \"alert_type\": 'accelerated',\n",
        "#                     \"details\": \"Velocity status changed: accelerated\"\n",
        "#                 }\n",
        "#             elif decel_cond.iloc[i] and count_velocity_maintained.iloc[i] < count_velocity_loss.iloc[i]:\n",
        "#                 self.df.at[i,'alerts']['momentum_alert'] = {\n",
        "#                     \"date\": self.df.iloc[i]['date'], \n",
        "#                     \"alert_type\": 'decelerated',\n",
        "#                     \"details\": \"Velocity status changed: decelerated\"\n",
        "#                 }\n",
        "\n",
        "#     def velocity_alert_dict(self, window=3):\n",
        "#         # Calculate conditions vectorized\n",
        "#         above_13ema = (self.df['close'] > self.df[['13ema','8ema']].max(axis=1))\n",
        "#         above_169ema = (self.df['close'] > self.df[['169ema','144ema']].max(axis=1))\n",
        "#         below_13ema = (self.df['close'] < self.df['13ema'])\n",
        "#         below_169ema = (self.df['close'] < self.df['169ema'])\n",
        "\n",
        "#         lng_term_max = self.df[['144ema','169ema']].max(axis=1)\n",
        "#         lng_term_min = self.df[['144ema','169ema']].min(axis=1)\n",
        "#         short_term_max = self.df[['13ema','8ema']].max(axis=1)\n",
        "#         short_term_min = self.df[['13ema','8ema']].min(axis=1)\n",
        "\n",
        "#         # Handle NaN values\n",
        "#         mask_nan = self.df['144ema'].isna()\n",
        "#         above_169ema[mask_nan] = above_13ema[mask_nan]\n",
        "#         below_169ema[mask_nan] = below_13ema[mask_nan]\n",
        "#         lng_term_max[mask_nan] = lng_term_min[mask_nan] = self.df['13ema'][mask_nan]\n",
        "#         short_term_max[mask_nan] = short_term_min[mask_nan] = self.df['8ema'][mask_nan]\n",
        "\n",
        "#         between_13_169ema = (self.df['13ema'] >= self.df['close']) & (self.df['close'] >= self.df['169ema'])\n",
        "#         in_up_trend = short_term_min > lng_term_max\n",
        "#         in_down_trend = short_term_max < lng_term_min\n",
        "\n",
        "#         # Apply alerts based on conditions\n",
        "#         for i in range(len(self.df)):\n",
        "#             if above_13ema.iloc[i] and above_169ema.iloc[i] and in_up_trend.iloc[i]:\n",
        "#                 alert_type = 'velocity_maintained'\n",
        "#             elif between_13_169ema.iloc[i] and below_13ema.iloc[i]:\n",
        "#                 alert_type = 'velocity_weak'\n",
        "#             elif below_169ema.iloc[i] and below_13ema.iloc[i]:\n",
        "#                 alert_type = 'velocity_loss'\n",
        "#             else:\n",
        "#                 alert_type = 'velocity_negotiating'\n",
        "\n",
        "#             self.df.at[i,'alerts']['velocity_alert'] = {\n",
        "#                 \"date\": self.df.iloc[i]['date'],\n",
        "#                 \"alert_type\": alert_type,\n",
        "#                 \"details\": f\"Velocity alert triggered: {alert_type}\"\n",
        "#             }\n",
        "\n",
        "#     def add_169ema_touch_alert(self):\n",
        "#         tolerance_dict = {interval: tolerance for interval, tolerance in \n",
        "#                         zip([1, 3, 5, 8, 13], [0.002, 0.02, 0.05, 0.07, 0.1])}\n",
        "\n",
        "#         # Calculate bounds vectorized\n",
        "#         lower_bound = self.df[['144ema','169ema']].min(axis=1) * (1 - tolerance_dict[self.interval])\n",
        "#         upper_bound = self.df[['144ema','169ema']].max(axis=1) * (1 + tolerance_dict[self.interval])\n",
        "\n",
        "#         # Handle missing EMAs\n",
        "#         mask_missing = self.df[['144ema','169ema']].isna().all(axis=1)\n",
        "#         lower_bound[mask_missing] = self.df[['13ema','8ema']].min(axis=1) * (1 - tolerance_dict[self.interval])\n",
        "#         upper_bound[mask_missing] = self.df[['13ema','8ema']].max(axis=1) * (1 + tolerance_dict[self.interval])\n",
        "\n",
        "#         # Calculate touch conditions\n",
        "#         touch_cond = ((lower_bound <= self.df['low']) & (self.df['low'] <= upper_bound)) | \\\n",
        "#                     ((lower_bound <= self.df['13ema']) & (self.df['13ema'] <= upper_bound)) | \\\n",
        "#                     ((lower_bound <= self.df['8ema']) & (self.df['8ema'] <= upper_bound))\n",
        "\n",
        "#         # Calculate support/resistance conditions\n",
        "#         lng_term_tunnel_max = self.df[['169ema','144ema']].max(axis=1)\n",
        "#         lng_term_tunnel_min = self.df[['169ema','144ema']].min(axis=1)\n",
        "#         short_term_tunnel_max = self.df[['13ema','8ema']].max(axis=1)\n",
        "#         short_term_tunnel_min = self.df[['13ema','8ema']].min(axis=1)\n",
        "#         candle_min = self.df[['close','open']].min(axis=1)\n",
        "#         candle_max = self.df[['close','open']].max(axis=1)\n",
        "\n",
        "#         # Apply alerts where touch conditions are met\n",
        "#         for i in range(len(self.df)):\n",
        "#             if touch_cond.iloc[i]:\n",
        "#                 if short_term_tunnel_min.iloc[i] > lng_term_tunnel_max.iloc[i] and candle_min.iloc[i] > lng_term_tunnel_min.iloc[i]:\n",
        "#                     alert_type = 'support'\n",
        "#                 elif short_term_tunnel_max.iloc[i] < lng_term_tunnel_max.iloc[i] and self.df.iloc[i]['close'] < lng_term_tunnel_max.iloc[i]:\n",
        "#                     alert_type = 'resistance'\n",
        "#                 else:\n",
        "#                     continue\n",
        "\n",
        "#                 self.df.at[i,'alerts']['169ema_touched'] = {\n",
        "#                     \"date\": self.df.iloc[i]['date'],\n",
        "#                     \"type\": alert_type,\n",
        "#                     \"count\": 1,  # Simplified count\n",
        "#                     \"169ema\": self.df.iloc[i]['169ema'],\n",
        "#                     \"details\": f\"Close price touched EMA169 from {alert_type} within {tolerance_dict[self.interval] * 100}% tolerance\"\n",
        "#                 }\n",
        "\n",
        "#     def filter_data(self):\n",
        "#         filtered_data = {self.symbol: self.df[['date','interval','alerts']].to_dict('records')}\n",
        "#         return filtered_data\n",
        "\n",
        "#     def apply(self):\n",
        "#         self.add_169ema_touch_alert()\n",
        "#         self.velocity_alert_dict()\n",
        "#         self.velocity_accel_decel_alert()\n",
        "#         self.filter_data()\n",
        "#         return self.df.to_dict('records')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AddStructuralArea:\n",
        "    def __init__(self, df_dict, interval=1):\n",
        "        self.df_dict = [entry for entry in df_dict if entry['interval'] == interval]\n",
        "        self.window = 30\n",
        "\n",
        "    def _ensure_structural_area(self, entry):\n",
        "        if 'structural_area' not in entry:\n",
        "            entry['structural_area'] = {}\n",
        "\n",
        "    def _get_window_data(self, i):\n",
        "        window = self.df_dict[i-self.window:i]\n",
        "        return {\n",
        "            'close': [entry['close'] for entry in window],\n",
        "            'high': max(entry['high'] for entry in window),\n",
        "            'low': min(entry['low'] for entry in window)\n",
        "        }\n",
        "\n",
        "    def kernel_density_estimation(self):\n",
        "        for i, entry in enumerate(self.df_dict):\n",
        "            if i < self.window:\n",
        "                continue\n",
        "                \n",
        "            self._ensure_structural_area(entry)\n",
        "            window_data = self._get_window_data(i)\n",
        "            close_prices = np.array(window_data['close'])\n",
        "\n",
        "            # Calculate histogram\n",
        "            hist_counts, bin_edges = np.histogram(close_prices, bins=20)\n",
        "            max_bin_idx = np.argmax(hist_counts)\n",
        "            \n",
        "            # Find second highest peak\n",
        "            remaining_hist = hist_counts[max_bin_idx+1:]\n",
        "            sec_bin_idx = max_bin_idx + 1 + np.argmax(remaining_hist) if len(remaining_hist) > 0 else max_bin_idx\n",
        "\n",
        "            # Get bin ranges\n",
        "            main_bin = (bin_edges[max_bin_idx], bin_edges[max_bin_idx + 1])\n",
        "            second_bin = (bin_edges[sec_bin_idx], bin_edges[sec_bin_idx + 1])\n",
        "\n",
        "            entry['structural_area']['kernel_density_estimation'] = {\n",
        "                \"date\": entry['date'],\n",
        "                \"top\": main_bin[0],\n",
        "                \"bottom\": main_bin[1],\n",
        "                \"second_top\": second_bin[0],\n",
        "                \"second_bottom\": second_bin[1],\n",
        "                \"details\": f\"Most frequent price bin between {main_bin[0]} and {main_bin[1]}.\\nSecond most frequent price bin between {second_bin[0]} and {second_bin[1]}.\"\n",
        "            }\n",
        "\n",
        "    def fibonacci_retracement(self):\n",
        "        for i, entry in enumerate(self.df_dict):\n",
        "            if i < self.window:\n",
        "                continue\n",
        "                \n",
        "            self._ensure_structural_area(entry)\n",
        "            window_data = self._get_window_data(i)\n",
        "            \n",
        "            # Calculate fibonacci levels\n",
        "            diff = window_data['high'] - window_data['low']\n",
        "            fibs = {\n",
        "                'fib_236': 0.236,\n",
        "                'fib_382': 0.382,\n",
        "                'fib_500': 0.500,\n",
        "                'fib_618': 0.618,\n",
        "                'fib_786': 0.,\n",
        "                'fib_1236': 1.236,\n",
        "                'fib_1382': 1.382\n",
        "            }\n",
        "            \n",
        "            levels = {key: window_data['low'] + (diff * ratio) \n",
        "                    for key, ratio in fibs.items()}\n",
        "            \n",
        "            entry['structural_area']['fibonacci_retracement'] = {\n",
        "                \"date\": entry['date'],\n",
        "                **levels,\n",
        "                \"details\": f\"Fibonacci retracement levels: \" + \n",
        "                        \", \".join(f\"{ratio*100}%: {level}\" \n",
        "                                for ratio, level in zip(fibs.values(), levels.values()))\n",
        "            }\n",
        "\n",
        "    def apply(self):\n",
        "        self.kernel_density_estimation()\n",
        "        self.fibonacci_retracement()\n",
        "        return self.df_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataVisualizer:\n",
        "    def __init__(self, mongo_config):\n",
        "        \"\"\"Initialize the DataAnalyzer class\"\"\"\n",
        "        self.mongo_config = mongo_config\n",
        "\n",
        "        self.host = self.mongo_config['connection_string']\n",
        "        self.db_name = self.mongo_config['db']\n",
        "        self.collection_name = self.mongo_config['processed_collection_name']\n",
        "        \n",
        "    def fetch_and_prepare_data(self, symbol):\n",
        "        \"\"\"\n",
        "        Fetch and prepare data for a given stock symbol.\n",
        "        \n",
        "        Parameters: \n",
        "        symbol (str): The stock symbol to fetch data for.\n",
        "        \n",
        "        Returns:\n",
        "        list: A list of dictionaries containing the prepared data.\n",
        "        \"\"\"\n",
        "        # Fetch all interval data for the selected stock\n",
        "        lst = list(MongoClient(self.host)[self.db_name][self.collection_name]\\\n",
        "            .find(\n",
        "                {'symbol': symbol, 'instrument': {'$in': ['equity', 'crypto', 'commodity', 'bond', 'sector']}},  # Add instrument filter\n",
        "                {\n",
        "                    '_id': 0, \n",
        "                    'date': 1,  # Ensure date is included in projection\n",
        "                    'symbol': 1,  # Include symbol\n",
        "                    'close': 1, \n",
        "                    'open': 1, \n",
        "                    'low': 1, \n",
        "                    'high': 1, \n",
        "                    'interval': 1, \n",
        "                    '13ema': 1,\n",
        "                    '8ema': 1,\n",
        "                    '169ema': 1, \n",
        "                    '144ema': 1\n",
        "                }\n",
        "            ).sort([('interval', ASCENDING), ('date', ASCENDING)]))\n",
        "        \n",
        "        if not lst:\n",
        "            print(f\"No data found for symbol: {symbol}\")\n",
        "            return []\n",
        "            \n",
        "        return lst\n",
        "\n",
        "    def plot_candlestick_with_alerts(self, df, alert_df, alert, title=None, save_path=None):\n",
        "        # Create a candlestick chart\n",
        "        fig = go.Figure(data=[go.Candlestick(x=df['date'],\n",
        "                        open=df['open'], high=df['high'],\n",
        "                        low=df['low'], close=df['close'],\n",
        "                        name='Candlestick')])\n",
        "\n",
        "        # Plot the EMAs as lines\n",
        "        fig.add_scatter(x=df['date'], y=df['13ema'], mode='lines', name='13EMA')\n",
        "        fig.add_scatter(x=df['date'], y=df['8ema'], mode='lines', name='8EMA')\n",
        "        fig.add_scatter(x=df['date'], y=df['169ema'], mode='lines', name='169EMA')\n",
        "        fig.add_scatter(x=df['date'], y=df['144ema'], mode='lines', name='144EMA')\n",
        "        \n",
        "        # Add annotations for alerts\n",
        "        for i, row in alert_df.iterrows():\n",
        "            fig.add_annotation(\n",
        "                x=row['date'],\n",
        "                y=row['close'],\n",
        "                text=row[alert],\n",
        "                showarrow=True,\n",
        "                arrowhead=1\n",
        "            )\n",
        "\n",
        "        # Update the layout for better visualization\n",
        "        fig.update_layout(\n",
        "            title=title,\n",
        "            xaxis_title=\"Date\",\n",
        "            yaxis_title=\"Price\",\n",
        "            xaxis_rangeslider_visible=False\n",
        "        )\n",
        "        \n",
        "        # Save the plot if save_path is provided\n",
        "        if save_path:\n",
        "            fig.write_image(save_path)\n",
        "\n",
        "        # Show the plot\n",
        "        fig.show()\n",
        "        \n",
        "    def plot_candlestick_with_structural_area(self, df, price_levels, dense_trading_area, selected_date):\n",
        "        # Create a candlestick chart\n",
        "        fig = go.Figure(data=[go.Candlestick(x=df['date'],\n",
        "                        open=df['open'], high=df['high'],\n",
        "                        low=df['low'], close=df['close'],\n",
        "                        name='Candlestick')])\n",
        "\n",
        "        # Add Horizontal Lines for price levels with different colors\n",
        "        colors = ['rgba(255,0,0,0.3)', 'rgba(0,255,0,0.3)', 'rgba(0,0,255,0.3)', \n",
        "                'rgba(255,165,0,0.3)', 'rgba(128,0,128,0.3)']  # Red, Green, Blue, Orange, Purple\n",
        "        for i, level in enumerate(price_levels):\n",
        "            if pd.notna(level) and isinstance(level, (int, float)):  # Check if numeric\n",
        "                fig.add_hline(\n",
        "                    y=float(level),  # Ensure numeric type\n",
        "                    line_dash='solid', \n",
        "                    line_color=colors[i % len(colors)],\n",
        "                    annotation_text=f'fibonacci {float(level):.2f}',\n",
        "                    annotation_position='top right'\n",
        "                )\n",
        "\n",
        "        # Add Horizontal Lines for dense trading areas\n",
        "        print(\"Dense trading areas:\", dense_trading_area)  # Debug print\n",
        "        for area in dense_trading_area:\n",
        "            if pd.notna(area) and isinstance(area, (int, float)):  # Check if numeric\n",
        "                print(f\"Adding rectangle for area: {area}\")  # Debug print\n",
        "                # Add a horizontal dashed line\n",
        "                fig.add_hline(\n",
        "                    y=float(area),  # Ensure numeric type\n",
        "                    line_dash=\"dash\",\n",
        "                    line_color=\"rgba(255,0,0,0.3)\",  # Transparent red\n",
        "                    annotation_text=f'Dense Trading Area {float(area):.2f}',\n",
        "                    annotation_position='top right'\n",
        "                )\n",
        "\n",
        "        # Add a vertical line at the selected date\n",
        "        if isinstance(selected_date, (str, pd.Timestamp)):  # Ensure date type\n",
        "            # Convert selected_date to datetime if it's a string\n",
        "            selected_date = pd.to_datetime(selected_date)\n",
        "            \n",
        "            # Create shape for vertical line\n",
        "            fig.add_shape(\n",
        "                type=\"line\",\n",
        "                x0=selected_date,\n",
        "                x1=selected_date,\n",
        "                y0=0,\n",
        "                y1=1,\n",
        "                yref=\"paper\",\n",
        "                line=dict(color=\"black\", width=1, dash=\"solid\"),\n",
        "            )\n",
        "            \n",
        "            # Add annotation for the date\n",
        "            fig.add_annotation(\n",
        "                x=selected_date,\n",
        "                y=1,\n",
        "                yref=\"paper\",\n",
        "                text=f'Selected Date: {selected_date.strftime(\"%Y-%m-%d\")}',\n",
        "                showarrow=False,\n",
        "                yshift=10\n",
        "            )\n",
        "        # Update the layout for better visualization\n",
        "        fig.update_layout(title='Candlestick with Price Levels', xaxis_title='Date', yaxis_title='Price', xaxis_rangeslider_visible=False)\n",
        "        fig.show()\n",
        "    \n",
        "    def visualize_alerts(self, selected_symbol, alert_type, anchor_date='1990-01-01'):\n",
        "        \"\"\"\n",
        "        Analyze and plot alerts for a given symbol and alert type.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        selected_symbol : str\n",
        "            The stock symbol to analyze\n",
        "        alert_type : str\n",
        "            Type of alert to analyze ('momentum_alert' or '169ema_touched')\n",
        "        anchor_date : str, optional\n",
        "            Starting date for analysis, default '1990-01-01'\n",
        "        \"\"\"\n",
        "        df_dict = self.fetch_and_prepare_data(selected_symbol)\n",
        "        # Create output directory with timestamp\n",
        "        alert_name = 'momentum_alerts' if alert_type == 'momentum_alert' else 'support_resistance_alerts'\n",
        "        output_dir = os.path.join('..', 'eda_viz', f'{selected_symbol}_{alert_name}')\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Define intervals to analyze\n",
        "        intervals = [1, 3, 5, 8, 13]\n",
        "\n",
        "        # Create plots for each interval\n",
        "        for interval in intervals:\n",
        "            print(f\"\\nAnalyzing {interval}-day interval data\")\n",
        "            \n",
        "            # Filter data for current interval\n",
        "            fil_df_dict = [entry for entry in df_dict if entry['interval'] == interval]\n",
        "            add_alert = AddAlert(fil_df_dict, interval, selected_symbol)\n",
        "            # Generate alerts\n",
        "            alert_dict = add_alert.apply()\n",
        "            \n",
        "            # Extract alert types\n",
        "            for entry in alert_dict:\n",
        "                if 'alerts' in entry and alert_type in entry['alerts']:\n",
        "                    entry[alert_type] = entry['alerts'][alert_type]['alert_type' if alert_type == 'momentum_alert' else 'type']\n",
        "        \n",
        "            # Convert to DataFrame and filter dates\n",
        "            alert_dict_df = pd.DataFrame(alert_dict)\n",
        "            alert_dict_df = alert_dict_df[alert_dict_df['date'] > anchor_date]\n",
        "            \n",
        "            # Filter alerts by type\n",
        "            non_null_alerts = alert_dict_df[alert_dict_df[alert_type].notna()]\n",
        "            \n",
        "            # Create filtered DataFrames based on alert type\n",
        "            if alert_type == 'momentum_alert':\n",
        "                alert_1 = non_null_alerts[\n",
        "                    (non_null_alerts[alert_type] == 'accelerated') & \n",
        "                    (non_null_alerts['date'] > anchor_date)\n",
        "                ]\n",
        "                alert_2 = non_null_alerts[\n",
        "                    (non_null_alerts[alert_type] == 'decelerated') & \n",
        "                    (non_null_alerts['date'] > anchor_date)\n",
        "                ]\n",
        "            else:  # 169ema_touched\n",
        "                alert_1 = non_null_alerts[\n",
        "                    (non_null_alerts[alert_type] == 'support') & \n",
        "                    (non_null_alerts['date'] > anchor_date)\n",
        "                ]\n",
        "                alert_2 = non_null_alerts[\n",
        "                    (non_null_alerts[alert_type] == 'resistance') & \n",
        "                    (non_null_alerts['date'] > anchor_date)\n",
        "                ]\n",
        "            \n",
        "            # Plot both alert types in one plot\n",
        "            alert_desc = 'Momentum' if alert_type == 'momentum_alert' else 'Support/Resistance'\n",
        "            title = f\"{selected_symbol} {interval}-day Interval - {alert_desc} Alerts\"\n",
        "            filename = f\"{selected_symbol}_{interval}day_{alert_name}.png\"\n",
        "            save_path = os.path.join(output_dir, filename)\n",
        "            \n",
        "            print(f\"Plotting {alert_desc.lower()} alerts for {interval}-day interval\")\n",
        "            self.plot_candlestick_with_alerts(\n",
        "                alert_dict_df,\n",
        "                pd.concat([alert_1, alert_2]),\n",
        "                alert_type,\n",
        "                title=title\n",
        "            )\n",
        "    \n",
        "    def visualize_structural_area(self, selected_symbol, selected_date='2024-12-01'):\n",
        "        df_dict = self.fetch_and_prepare_data(selected_symbol)\n",
        "        intervals = [1, 3, 5, 8, 13]\n",
        "        for interval in intervals:\n",
        "            print(f\"\\nAnalyzing {interval}-day interval data\")\n",
        "            \n",
        "            # Filter data for current interval and symbol\n",
        "            fil_df_dict = [entry for entry in df_dict \n",
        "                        if entry['interval'] == interval \n",
        "                        and entry['symbol'] == selected_symbol]\n",
        "            \n",
        "            if not fil_df_dict:\n",
        "                print(f\"No data found for interval {interval}\")\n",
        "                continue\n",
        "                \n",
        "            add_struct_area = AddStructuralArea(fil_df_dict, interval)\n",
        "            struct_area_dict = add_struct_area.apply()\n",
        "            \n",
        "            # Convert to DataFrame first\n",
        "            for entry in struct_area_dict:\n",
        "                if 'structural_area' in entry:\n",
        "                    entry['top'] = entry['structural_area']['kernel_density_estimation']['top']\n",
        "                    entry['bottom'] = entry['structural_area']['kernel_density_estimation']['bottom']\n",
        "                    entry['second_top'] = entry['structural_area']['kernel_density_estimation']['second_top']\n",
        "                    entry['second_bottom'] = entry['structural_area']['kernel_density_estimation']['second_bottom']\n",
        "                    entry['fib_236'] = entry['structural_area']['fibonacci_retracement']['fib_236']\n",
        "                    entry['fib_382'] = entry['structural_area']['fibonacci_retracement']['fib_382']\n",
        "                    entry['fib_500'] = entry['structural_area']['fibonacci_retracement']['fib_500']\n",
        "                    entry['fib_618'] = entry['structural_area']['fibonacci_retracement']['fib_618']\n",
        "                    entry['fib_786'] = entry['structural_area']['fibonacci_retracement']['fib_786']\n",
        "                    entry['fib_1236'] = entry['structural_area']['fibonacci_retracement']['fib_1236']\n",
        "                    entry['fib_1382'] = entry['structural_area']['fibonacci_retracement']['fib_1382']\n",
        "                else:\n",
        "                    entry['touch_type'] = np.nan\n",
        "                    entry['count'] = 0.0\n",
        "            \n",
        "            # Convert to DataFrame\n",
        "            struct_area_df = pd.DataFrame(struct_area_dict).drop(columns=['structural_area'])\n",
        "            # Filter structural area data up to selected date\n",
        "            struct_area_df_filtered = struct_area_df[struct_area_df['date'] <= pd.to_datetime(selected_date)]\n",
        "\n",
        "            # Only plot if we have valid data\n",
        "            if not struct_area_df_filtered.empty and any(struct_area_df_filtered['top'].notna()):\n",
        "                print(f\"Plotting structural areas for {interval}-day interval up to {selected_date}\")                \n",
        "                # Plot the structural area using filtered data for structural areas but full data for candlesticks\n",
        "                print(struct_area_df_filtered['second_top'].iloc[-1], struct_area_df_filtered['second_bottom'].iloc[-1])\n",
        "                self.plot_candlestick_with_structural_area(\n",
        "                    struct_area_df,  # Use full data for candlesticks\n",
        "                    [struct_area_df_filtered['fib_236'].iloc[-1], struct_area_df_filtered['fib_382'].iloc[-1],\n",
        "                    struct_area_df_filtered['fib_500'].iloc[-1], struct_area_df_filtered['fib_618'].iloc[-1],\n",
        "                    struct_area_df_filtered['fib_786'].iloc[-1], struct_area_df_filtered['fib_1236'].iloc[-1],\n",
        "                    struct_area_df_filtered['fib_1382'].iloc[-1]],\n",
        "                    [struct_area_df_filtered['top'].iloc[-1], struct_area_df_filtered['bottom'].iloc[-1],\n",
        "                    struct_area_df_filtered['second_top'].iloc[-1], struct_area_df_filtered['second_bottom'].iloc[-1]],\n",
        "                    selected_date\n",
        "                )\n",
        "            else:\n",
        "                print(f\"No valid structural area data found for interval {interval} up to {selected_date}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the MongoDB configuration\n",
        "mongo_config = load_client_config('mongodb', 'production')\n",
        "\n",
        "# Initialize the DataVisualizer\n",
        "data_visualizer = DataVisualizer(mongo_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vegas Channels Alerts Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze Momentum Alerts\n",
        "data_visualizer.visualize_alerts('CL=F', 'momentum_alert')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vegas Channels Support/Resistance Alerts Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_visualizer.visualize_alerts('AMD', '169ema_touched')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Structural Area Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_visualizer = DataVisualizer(mongo_config)\n",
        "data_visualizer.visualize_structural_area('CL=F', selected_date='2024-12-23')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stock Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1. Fetch Alert Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-07T03:01:36.300768Z",
          "start_time": "2024-11-07T03:01:29.220387Z"
        }
      },
      "outputs": [],
      "source": [
        "# Initialize MongoClient with the URL\n",
        "mongo_client = MongoClient(mongo_config['connection_string'])\n",
        "\n",
        "# Load the processed data from the database\n",
        "processed_df = pd.DataFrame(list(mongo_client[mongo_config['db']][mongo_config['processed_collection_name']].find({})))\n",
        "\n",
        "# Load the alert data from the database\n",
        "alert_dict = list(mongo_client[mongo_config['db']][mongo_config['alert_collection_name']['long_term']]\\\n",
        "    .find({'instrument': {'$in': ['equity']}, \n",
        "        'date': {'$gte': pd.to_datetime('2020-01-01')}}))\n",
        "\n",
        "# Extract the alerts from the alert_dict and process fields\n",
        "for row in alert_dict:\n",
        "    if 'alerts' in row and 'momentum_alert' in row['alerts']:\n",
        "        row['momentum_alert'] = row['alerts']['momentum_alert']['alert_type']\n",
        "\n",
        "    if 'alerts' in row and 'velocity_alert' in row['alerts']:\n",
        "        row['velocity_alert'] = row['alerts']['velocity_alert']['alert_type']\n",
        "\n",
        "    if 'alerts' in row and '169ema_touched' in row['alerts']:\n",
        "        row['touch_type'] = row['alerts']['169ema_touched']['type']\n",
        "        row['count'] = row['alerts']['169ema_touched']['count']\n",
        "\n",
        "    elif 'alerts' in row and '13ema_touched' in row['alerts']:\n",
        "        row['touch_type'] = row['alerts']['13ema_touched']['type']\n",
        "        row['count'] = row['alerts']['13ema_touched']['count']\n",
        "\n",
        "    else:\n",
        "        row['touch_type'] = np.nan\n",
        "\n",
        "    if 'structural_area' in row and isinstance(row['structural_area'], dict):\n",
        "        if 'kernel_density_estimation' in row['structural_area']:\n",
        "            kde = row['structural_area']['kernel_density_estimation']\n",
        "            row['top'] = kde.get('top')\n",
        "            row['bottom'] = kde.get('bottom') \n",
        "            row['second_top'] = kde.get('second_top')\n",
        "            row['second_bottom'] = kde.get('second_bottom')\n",
        "            \n",
        "        if 'fibonacci_retracement' in row['structural_area']:\n",
        "            fib = row['structural_area']['fibonacci_retracement']\n",
        "            row['fib_236'] = fib.get('fib_236')\n",
        "            row['fib_382'] = fib.get('fib_382')\n",
        "            row['fib_500'] = fib.get('fib_500')\n",
        "            row['fib_618'] = fib.get('fib_618')\n",
        "            row['fib_786'] = fib.get('fib_786')\n",
        "            \n",
        "# Convert the alert_dict to a DataFrame and clean up\n",
        "alert_df = pd.DataFrame(alert_dict)\n",
        "alert_df = alert_df.drop(columns=['alerts', '_id'])\n",
        "alert_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2. One Hot Encoded the Alert Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the OneHotEncoder\n",
        "ohe = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "columns_to_encode = ['touch_type', 'momentum_alert']\n",
        "df_to_be_encoded = alert_df.loc[:,columns_to_encode]\n",
        "alert_df.drop(columns=columns_to_encode, inplace=True)\n",
        "\n",
        "encoded_array = ohe.fit_transform(df_to_be_encoded)\n",
        "encoded_df = pd.DataFrame(encoded_array, columns=ohe.get_feature_names_out())\n",
        "\n",
        "# Concat the encoded dataframe to the alert dataframe\n",
        "encoded_alert_df = pd.concat([alert_df, encoded_df], axis=1)\n",
        "encoded_alert_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3. Apply Interval Weighting on Encoded Alert Data\n",
        "\n",
        "* Alert on higher interval indicates a more convincing message\n",
        "* High Interval Alert lasts longer\n",
        "* High Interval Alerts dominates the equity movement in Low Interval\n",
        "    - Avoid False Signals in Low Interval\n",
        "    - High Interval Signals are Stable in a long run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the distinct interval\n",
        "distinct_intervals = encoded_alert_df['interval'].unique()\n",
        "# Apply Linear Weighting on Alert Data\n",
        "interval_weights = {interval: weight for weight, interval in enumerate(distinct_intervals, start=1)}\n",
        "encoded_alert_df['interval_weight'] = encoded_alert_df['interval'].map(interval_weights)\n",
        "\n",
        "evaluate_summary = encoded_alert_df.copy()\n",
        "# Calculate weighted values for each alert type\n",
        "alert_types = ['touch_type_resistance','touch_type_support', 'momentum_alert_accelerated','momentum_alert_decelerated']\n",
        "for alert in alert_types:\n",
        "    evaluate_summary[f'weighted_{alert}'] = evaluate_summary[alert] * evaluate_summary['interval_weight']\n",
        "    \n",
        "evaluate_summary = evaluate_summary.drop(columns=alert_types+['touch_type_nan','momentum_alert_nan','touch_type_neutral','interval_weight'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4. Data Analysis on Encoded Alert Data\n",
        "\n",
        "* Analyze all stocks daily\n",
        "* Group the data by the critical alerts\n",
        "    - Acceleration/ Deceleration\n",
        "    - Support/ Resistance Touching\n",
        "* Sort the grouped alert data by Momentum alert, touching alerts and touching counts \n",
        "    - Sort the Interval in Ascending order because we prefer potential to stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_sum = evaluate_summary[(evaluate_summary['date'] >= '2020-01-01')]\n",
        "# Data Analysis\n",
        "results = eval_sum \\\n",
        "                .loc[:, ['symbol',\n",
        "                        'interval',\n",
        "                        'weighted_momentum_alert_accelerated',\n",
        "                        'weighted_momentum_alert_decelerated',\n",
        "                        'weighted_touch_type_resistance',\n",
        "                        'weighted_touch_type_support',\n",
        "                        'count'\n",
        "                        ]] \\\n",
        "        .groupby(['symbol', 'interval']) \\\n",
        "        .sum() \\\n",
        "        .sort_values(['interval', 'weighted_momentum_alert_accelerated',\n",
        "                        'weighted_momentum_alert_decelerated',\n",
        "                        'weighted_touch_type_support',\n",
        "                        'count'],\n",
        "\n",
        "                        ascending=[True, False, True, False, False]).reset_index()\n",
        "\n",
        "# Store the accelerating stock\n",
        "short_acc_equ = results[(results['weighted_momentum_alert_accelerated'] > 1) \\\n",
        "                        & (results['weighted_momentum_alert_decelerated'] < 1) \\\n",
        "                        & (results['interval'] <= 3)].loc[:, 'symbol']\n",
        "\n",
        "lng_acc_equ = results[(results['weighted_momentum_alert_accelerated'] > 1) \\\n",
        "                        & (results['weighted_momentum_alert_decelerated'] < 1) \\\n",
        "                        & (results['interval'] == 5)].loc[:, 'symbol']\n",
        "\n",
        "ext_lng_acc_equ = results[(results['weighted_momentum_alert_accelerated'] > 0) \\\n",
        "                        & (results['weighted_momentum_alert_decelerated'] < 1) \\\n",
        "                        & (results['interval'] == 13)].loc[:, 'symbol']\n",
        "\n",
        "# Store the main force accumulating stock\n",
        "short_main_acc_equ = results[(results['weighted_touch_type_support'] > 1) \\\n",
        "                                & (results['weighted_touch_type_resistance'] < 1) \\\n",
        "                                & (results['count'] > 2) \\\n",
        "                                & (results['interval'] <= 3)].loc[:, 'symbol']\n",
        "\n",
        "lng_main_acc_equ = results[(results['weighted_touch_type_support'] > 1) \\\n",
        "                        & (results['weighted_touch_type_resistance'] < 1) \\\n",
        "                        & (results['count'] > 2) \\\n",
        "                        & (results['interval'] == 5)].loc[:, 'symbol']\n",
        "\n",
        "ext_lng_main_acc_equ = results[(results['weighted_touch_type_support'] > 1) \\\n",
        "                                & (results['weighted_touch_type_resistance'] < 1) \\\n",
        "                                & (results['count'] > 2) \\\n",
        "                                & (results['interval'] == 13)].loc[:, 'symbol']\n",
        "\n",
        "# Create dictionary of results\n",
        "stock_dict = {\n",
        "        'date': str(eval_sum['date'].iloc[0]),\n",
        "        'accelerating': short_acc_equ.tolist(),\n",
        "        'main_accumulating': short_main_acc_equ.tolist(),\n",
        "        'long_accelerating': lng_acc_equ.tolist(),\n",
        "        'long_main_accumulating': lng_main_acc_equ.tolist(),\n",
        "        'ext_long_accelerating': ext_lng_acc_equ.tolist(),\n",
        "        'ext_accumulating': ext_lng_main_acc_equ.tolist(),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Finalized Stock Picking Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-12T19:04:41.970083Z",
          "start_time": "2024-11-12T19:04:36.730088Z"
        }
      },
      "outputs": [],
      "source": [
        "class Pick_Stock:\n",
        "    \"\"\"A class to pick stocks based on various criteria and alerts.\n",
        "    \n",
        "    Attributes:\n",
        "        data (pd.DataFrame): DataFrame containing stock data and alerts. Initialized as None and populated in run().\n",
        "        distinct_intervals (np.array): Array of unique intervals from data. Populated after data is loaded.\n",
        "        stock_candidates (dict): Dictionary to store candidate stocks.\n",
        "        interval_weights (dict): Weights assigned to different intervals.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mongo_config, instrument, start_date=None, sandbox_mode=False):\n",
        "        # Store MongoDB configuration\n",
        "        self.mongo_config = mongo_config\n",
        "\n",
        "        # Set Sandbox mode and start date accordingly\n",
        "        self.sandbox_mode = sandbox_mode\n",
        "        self.start_date = start_date if self.sandbox_mode else '2020-01-01'\n",
        "        self.instrument = instrument\n",
        "\n",
        "        # Connect to MongoDB\n",
        "        self.client = MongoClient(self.mongo_config['connection_string'])\n",
        "        self.db = self.client[self.mongo_config['db']]\n",
        "        self.collection = self.db[self.mongo_config['alert_collection_name']['long_term']]\n",
        "        self.candidate_collection_name = self.mongo_config['candidates_collection_name']['long_term']\n",
        "        self.candidate_collection = self.db[self.candidate_collection_name]\n",
        "        # Initialize a one hot encoder\n",
        "        self.ohe = OneHotEncoder(sparse_output=False)\n",
        "    \n",
        "    def get_stock_dataframe(self):\n",
        "        # Fetch the data from MongoDB and convert to DataFrame\n",
        "        alert_dict = list(self.collection.find(\n",
        "            {\n",
        "                'date': {'$gte': pd.to_datetime(self.start_date)},\n",
        "                'instrument': self.instrument\n",
        "                },\n",
        "            {'_id': 0},\n",
        "            sort=[('date', 1)]\n",
        "        ))\n",
        "        # Extract the alerts from the alert_dict and process fields\n",
        "        for row in alert_dict:\n",
        "            if 'alerts' in row and 'momentum_alert' in row['alerts']:\n",
        "                row['momentum_alert'] = row['alerts']['momentum_alert']['alert_type']\n",
        "\n",
        "            if 'alerts' in row and 'velocity_alert' in row['alerts']:\n",
        "                row['velocity_alert'] = row['alerts']['velocity_alert']['alert_type']\n",
        "\n",
        "            if 'alerts' in row and '169ema_touched' in row['alerts']:\n",
        "                row['touch_type'] = row['alerts']['169ema_touched']['type']\n",
        "                row['count'] = row['alerts']['169ema_touched']['count']\n",
        "\n",
        "            elif 'alerts' in row and '13ema_touched' in row['alerts']:\n",
        "                row['touch_type'] = row['alerts']['13ema_touched']['type']\n",
        "                row['count'] = row['alerts']['13ema_touched']['count']\n",
        "\n",
        "            elif 'alerts' in row and 'velocity_alert' in row['alerts']:\n",
        "                row['velocity_alert'] = row['alerts']['velocity_alert']['alert_type']\n",
        "\n",
        "            else:\n",
        "                row['touch_type'] = np.nan\n",
        "                \n",
        "            # Extract the structural area data\n",
        "            if 'structural_area' in row and isinstance(row['structural_area'], dict):\n",
        "                if 'kernel_density_estimation' in row['structural_area']:\n",
        "                    kde = row['structural_area']['kernel_density_estimation']\n",
        "                    row['top'] = kde.get('top')\n",
        "                    row['bottom'] = kde.get('bottom') \n",
        "                    row['second_top'] = kde.get('second_top')\n",
        "                    row['second_bottom'] = kde.get('second_bottom')\n",
        "            \n",
        "                if 'fibonacci_retracement' in row['structural_area']:\n",
        "                    fib = row['structural_area']['fibonacci_retracement']\n",
        "                    row['fib_236'] = fib.get('fib_236')\n",
        "                    row['fib_382'] = fib.get('fib_382')\n",
        "                    row['fib_500'] = fib.get('fib_500')\n",
        "                    row['fib_618'] = fib.get('fib_618')\n",
        "                    row['fib_786'] = fib.get('fib_786')\n",
        "            \n",
        "        # Convert the alert_dict to a DataFrame and process it\n",
        "        data = pd.DataFrame(alert_dict).drop(columns=['alerts'], errors='ignore')\n",
        "\n",
        "        # Prepare the data for encoding\n",
        "        alert_columns = ['touch_type', 'momentum_alert', 'velocity_alert']\n",
        "        encoded_arr = self.ohe.fit_transform(data[alert_columns])\n",
        "        encoded_df = pd.DataFrame(encoded_arr, columns=self.ohe.get_feature_names_out())\n",
        "\n",
        "        # Concat the encoded DataFrame with the original DataFrame\n",
        "        data = pd.concat([data.drop(columns=alert_columns), encoded_df], axis=1)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def create_time_series_collection(self, collection_name, keep_duration=None):\n",
        "        if collection_name not in self.db.list_collection_names():\n",
        "            self.db.create_collection(\n",
        "                collection_name,\n",
        "                timeseries={\n",
        "                    \"timeField\": \"date\" if 'datastream' not in collection_name else \"datetime\",\n",
        "                    \"metaField\": \"symbol\",\n",
        "                    \"granularity\": \"hours\" if 'datastream' not in collection_name else \"minutes\"\n",
        "                },\n",
        "                expireAfterSeconds=keep_duration\n",
        "            )\n",
        "            print(f\"Time Series Collection {collection_name} created successfully\")\n",
        "\n",
        "    def insert_candidates(self, candidates):\n",
        "        \"\"\"\n",
        "        Insert candidate stock data into a MongoDB time series collection, setting up auto-expiry for 7 days.\n",
        "        Only insert data for dates newer than the latest existing date.\n",
        "        \"\"\"\n",
        "        # Create the time series collection if it doesn't exist\n",
        "        keep_duration = 157788000 # 5 years\n",
        "        self.create_time_series_collection(self.candidate_collection_name, keep_duration=keep_duration)\n",
        "\n",
        "        # Get latest date from collection\n",
        "        latest_record = self.candidate_collection.find_one({'instrument': self.instrument}, sort=[(\"date\", -1)])\n",
        "        latest_date = latest_record['date'] if latest_record else None\n",
        "\n",
        "        # Prepare data for insertion, filtering for dates newer than latest\n",
        "        documents = []\n",
        "        for date_str, values in candidates.items():\n",
        "            date_obj = pd.to_datetime(date_str)\n",
        "            \n",
        "            # Skip if date is not newer than latest\n",
        "            if latest_date and date_obj <= latest_date:\n",
        "                continue\n",
        "                \n",
        "            documents.append({\n",
        "                'date': date_obj,\n",
        "                'instrument': self.instrument,\n",
        "                'accelerating': values['accelerating'], \n",
        "                'accumulating': values['accumulating'],\n",
        "                'long_accelerating': values['long_accelerating'],\n",
        "                'long_accumulating': values['long_accumulating'],\n",
        "                'velocity_maintained': values['velocity_maintained']\n",
        "            })\n",
        "\n",
        "        # Insert documents into MongoDB\n",
        "        if documents:\n",
        "            try:\n",
        "                self.candidate_collection.insert_many(documents, ordered=False)\n",
        "                print(\"New candidate stocks inserted successfully!\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error inserting documents: {e}\")\n",
        "        else:\n",
        "            print(\"No new data to insert. All candidate data is up to date.\")\n",
        "        \n",
        "    def evaluate_micro_interval_stocks(self, data):\n",
        "        # Group by symbol and apply interval-based weighting for velocity alerts\n",
        "        eval_sum = data.copy()\n",
        "\n",
        "        # Apply Linear Weighting on Alert Data\n",
        "        eval_sum['interval_weight'] = eval_sum['interval'].map(self.interval_weights)\n",
        "        \n",
        "        # Calculate weighted values for each alert type\n",
        "        alerts_cols = ['touch_type_resistance', 'touch_type_support', 'momentum_alert_accelerated', 'momentum_alert_decelerated']\n",
        "        for alert in alerts_cols:\n",
        "            eval_sum[f'weighted_{alert}'] = eval_sum[alert] * eval_sum['interval_weight'] \n",
        "        \n",
        "        # Drop the columns that are not needed\n",
        "        eval_sum = eval_sum.drop(\n",
        "            columns=alerts_cols + ['touch_type_nan', 'momentum_alert_nan', 'touch_type_neutral', 'interval_weight'])\n",
        "        \n",
        "        # Data Analysis\n",
        "        results = eval_sum \\\n",
        "                    .loc[:, ['symbol',\n",
        "                            'interval',\n",
        "                            'weighted_momentum_alert_accelerated',\n",
        "                            'weighted_momentum_alert_decelerated',\n",
        "                            'weighted_touch_type_resistance',\n",
        "                            'weighted_touch_type_support',\n",
        "                            'count'\n",
        "                            ]] \\\n",
        "            .groupby(['symbol', 'interval']) \\\n",
        "            .sum() \\\n",
        "            .sort_values(['interval', \n",
        "                        'weighted_momentum_alert_accelerated',\n",
        "                        'weighted_momentum_alert_decelerated',\n",
        "                        'weighted_touch_type_support',\n",
        "                        'count'],\n",
        "                        \n",
        "                        ascending=[True, False, True, False, False]).reset_index()\n",
        "\n",
        "        # Store the accelerating stock\n",
        "        short_acc_equ = results[(results['weighted_momentum_alert_accelerated'] > 1) \\\n",
        "                                & (results['weighted_momentum_alert_decelerated'] < 1) \\\n",
        "                                & (results['interval'] <= 3)].loc[:, 'symbol']\n",
        "        \n",
        "        lng_acc_equ = results[(results['weighted_momentum_alert_accelerated'] > 1) \\\n",
        "                            & (results['weighted_momentum_alert_decelerated'] < 1) \\\n",
        "                            & (results['interval'] == 5)].loc[:, 'symbol']\n",
        "        \n",
        "        # Store the main force accumulating stock\n",
        "        short_main_acc_equ = results[(results['weighted_touch_type_support'] > 1) \\\n",
        "                                    & (results['weighted_touch_type_resistance'] < 1) \\\n",
        "                                    & (results['count'] >= 2) \\\n",
        "                                    & (results['interval'] <= 3)].loc[:, 'symbol']\n",
        "        \n",
        "        lng_main_acc_equ = results[(results['weighted_touch_type_support'] > 1) \\\n",
        "                                & (results['weighted_touch_type_resistance'] < 1) \\\n",
        "                                & (results['count'] >= 1) \\\n",
        "                                & (results['interval'] == 5)].loc[:, 'symbol']\n",
        "\n",
        "        # Create dictionary of results\n",
        "        stock_dict = {\n",
        "            'accelerating': short_acc_equ.tolist(),\n",
        "            'accumulating': short_main_acc_equ.tolist(),\n",
        "            'long_accelerating': lng_acc_equ.tolist(),\n",
        "            'long_accumulating': lng_main_acc_equ.tolist()\n",
        "        }\n",
        "        \n",
        "        return stock_dict\n",
        "    \n",
        "    def evaluate_macro_interval_stocks(self, data):\n",
        "        # Group by symbol and apply interval-based weighting for velocity alerts\n",
        "        eval_sum = data.copy()\n",
        "        # The interval_weights dict has values as keys and indices as values, need to swap\n",
        "        self.interval_weights = {v: k for k, v in self.interval_weights.items()}\n",
        "        eval_sum['interval_weight'] = eval_sum['interval'].map(self.interval_weights)\n",
        "        \n",
        "        # Calculate weighted values for velocity alerts\n",
        "        eval_sum['weighted_velocity_maintained'] = eval_sum['velocity_alert_velocity_maintained'] * eval_sum['interval_weight']\n",
        "        eval_sum['weighted_velocity_weak'] = eval_sum['velocity_alert_velocity_weak'] * eval_sum['interval_weight']\n",
        "        eval_sum['weighted_velocity_loss'] = eval_sum['velocity_alert_velocity_loss'] * eval_sum['interval_weight']\n",
        "        \n",
        "        # Drop unnecessary columns\n",
        "        eval_sum = eval_sum.drop(columns=['velocity_alert_velocity_maintained', 'velocity_alert_velocity_weak', 'velocity_alert_velocity_loss', 'interval_weight'])\n",
        "        \n",
        "        # Group and aggregate data\n",
        "        results = eval_sum \\\n",
        "                    .loc[:, ['symbol',\n",
        "                            'interval',\n",
        "                            'weighted_velocity_maintained',\n",
        "                            'weighted_velocity_weak', \n",
        "                            'weighted_velocity_loss']] \\\n",
        "            .groupby(['symbol','interval']) \\\n",
        "            .sum() \\\n",
        "            .sort_values(['weighted_velocity_maintained',\n",
        "                        'weighted_velocity_weak',\n",
        "                        'weighted_velocity_loss'],\n",
        "                        ascending=[False, True, True]) \\\n",
        "            .reset_index()\n",
        "        \n",
        "        # Filter for stocks with maintained velocity in higher intervals (>=8)\n",
        "        maintained_stocks = results[(results['weighted_velocity_maintained'] > 0) &\n",
        "                                (results['weighted_velocity_weak'] == 0) &\n",
        "                                (results['weighted_velocity_loss'] == 0) &\n",
        "                                (results['interval'] >= 8)].loc[:, 'symbol']\n",
        "        # Create dictionary of results\n",
        "        stock_dict = {\n",
        "            'velocity_maintained': maintained_stocks.tolist()\n",
        "        }\n",
        "        \n",
        "        return stock_dict\n",
        "    \n",
        "    def run(self):\n",
        "        # Fetch and process data\n",
        "        self.data = self.get_stock_dataframe()\n",
        "        self.distinct_intervals = self.data['interval'].unique()\n",
        "        self.interval_weights = dict(enumerate(self.distinct_intervals, 1))\n",
        "        # Group data by date for more efficient processing\n",
        "        grouped_data = self.data.groupby('date')\n",
        "        \n",
        "        # Process candidates for each date in parallel\n",
        "        candidates_dict_micro = {}\n",
        "        candidates_dict_macro = {}\n",
        "        print('generating stock candidates for each day...')\n",
        "        \n",
        "        for date, today_data in grouped_data:\n",
        "            # Analyze candidates for this date\n",
        "            micro_interval_data = today_data.loc[today_data['interval'] <= 5]\n",
        "            candidates_dict_micro[str(date)] = self.evaluate_micro_interval_stocks(micro_interval_data)\n",
        "        \n",
        "            macro_interval_data = today_data.loc[today_data['interval'] >= 8]\n",
        "            candidates_dict_macro[str(date)] = self.evaluate_macro_interval_stocks(macro_interval_data)\n",
        "            \n",
        "        micro_interval_df = pd.DataFrame.from_dict(candidates_dict_micro, orient='index')\n",
        "        macro_interval_df = pd.DataFrame.from_dict(candidates_dict_macro, orient='index')\n",
        "            \n",
        "        stock_candidates_df = pd.concat([micro_interval_df, macro_interval_df], axis=1)\n",
        "        # # Store results in MongoDB\n",
        "        # self.insert_candidates(stock_candidates_df.to_dict(orient='index'))\n",
        "        return stock_candidates_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pick_stock_instance = Pick_Stock(mongo_config, instrument='crypto', sandbox_mode=True, start_date='2020-01-01')\n",
        "stock_candidates_df = pick_stock_instance.run()\n",
        "\n",
        "# Summary of the stock_candidates_df\n",
        "accelerating_count = stock_candidates_df['accelerating'].apply(lambda x: len(set(x))).sum()\n",
        "accumulating_count = stock_candidates_df['accumulating'].apply(lambda x: len(set(x))).sum()\n",
        "long_accelerating_count = stock_candidates_df['long_accelerating'].apply(lambda x: len(set(x))).sum()\n",
        "long_accumulating_count = stock_candidates_df['long_accumulating'].apply(lambda x: len(set(x))).sum()\n",
        "velocity_maintained_count = stock_candidates_df['velocity_maintained'].apply(lambda x: len(set(x))).sum()\n",
        "\n",
        "# Return the date range of the stock_candidates_df\n",
        "date_range = stock_candidates_df.index.unique()\n",
        "print(f'The date range of the stock_candidates_df is from {date_range.min()} to {date_range.max()}')\n",
        "\n",
        "print(f'The number of accelerating stocks is {accelerating_count}')\n",
        "print(f'The number of accumulating stocks is {accumulating_count}')\n",
        "print(f'The number of long accelerating stocks is {long_accelerating_count}')\n",
        "print(f'The number of long accumulating stocks is {long_accumulating_count}')\n",
        "print(f'The number of velocity maintained stocks is {velocity_maintained_count}')\n",
        "\n",
        "stock_candidates_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trading Strategy Design\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LongTermTradingStrategy:\n",
        "    def __init__(self, mongo_config, \n",
        "                instrument,\n",
        "                start_date=None,\n",
        "                end_date=None,\n",
        "                sandbox_mode=False,\n",
        "                initial_capital=10000,\n",
        "                buy_signals = None,\n",
        "                sell_signals = None,\n",
        "                verbose=False):\n",
        "        # Initialize trade tracking state\n",
        "        self._init_trade_state(instrument, buy_signals, sell_signals)\n",
        "        \n",
        "        # Initialize configuration parameters\n",
        "        self._init_config(mongo_config, instrument, start_date, end_date, sandbox_mode, initial_capital, verbose)\n",
        "        \n",
        "        # Initialize MongoDB connections and load data\n",
        "        self._init_mongo_connections()\n",
        "        self._load_historical_data()\n",
        "        self._load_alert_data()\n",
        "        \n",
        "    def _init_trade_state(self, instrument, buy_signals, sell_signals):\n",
        "        \"\"\"Initialize trade tracking variables\"\"\"\n",
        "        self.protected = False\n",
        "        self.peak_profit_pct = None  \n",
        "        self.peak_profit = None \n",
        "        self.trades = []\n",
        "        self.complete_trades = {}\n",
        "        self.current_trade = {}\n",
        "        self.dynamic_protection = False\n",
        "        self.buy_fee = 0.0025 if instrument == 'crypto' else 0.002\n",
        "        self.sell_fee = 0.0075 if instrument == 'crypto' else 0.002\n",
        "        self.dynamic_asset_control = False\n",
        "        self.buy_signals = buy_signals\n",
        "        self.sell_signals = sell_signals\n",
        "        \n",
        "    def _init_config(self, mongo_config, instrument, start_date, end_date, sandbox_mode, initial_capital, verbose):\n",
        "        \"\"\"Initialize configuration parameters\"\"\"\n",
        "        self.start_date = start_date if sandbox_mode else '2020-01-01'\n",
        "        self.end_date = end_date if sandbox_mode else '2024-12-31'\n",
        "        self.instrument = instrument\n",
        "        self.capital = initial_capital\n",
        "        self.initial_capital = initial_capital\n",
        "        self.daily_capital = initial_capital\n",
        "        self.mongo_config = mongo_config\n",
        "        self.verbose = verbose\n",
        "        \n",
        "    def _init_mongo_connections(self):\n",
        "        \"\"\"Initialize MongoDB client and database connections\"\"\"\n",
        "        self.client = MongoClient(self.mongo_config['connection_string'])\n",
        "        self.db = self.client[self.mongo_config['db']]\n",
        "        self.data_collection = self.db[self.mongo_config['processed_collection_name']]\n",
        "        self.alert_collection = self.db[self.mongo_config['alert_collection_name']['long_term']]\n",
        "        \n",
        "        # Load stock candidates data\n",
        "        self.stock_candidates = stock_candidates_df\n",
        "\n",
        "    def _load_historical_data(self):\n",
        "        \"\"\"Load historical price data from MongoDB\"\"\"\n",
        "        self.df = pd.DataFrame(list(self.data_collection.find(\n",
        "            {\n",
        "                'date': {'$gte': pd.to_datetime(self.start_date), '$lte': pd.to_datetime(self.end_date)},\n",
        "                'instrument': self.instrument,\n",
        "                'interval': 1\n",
        "            },\n",
        "            {'_id': 0}\n",
        "        ))).sort_values(by=['date', 'symbol'])\n",
        "\n",
        "    def _load_alert_data(self):\n",
        "        \"\"\"Load and process alert data from MongoDB\"\"\"\n",
        "        self.alert_df = self.get_alert_dataframe()\n",
        "\n",
        "    def get_alert_dataframe(self):\n",
        "        \"\"\"Fetch and process alert data into DataFrame format\"\"\"\n",
        "        data_dict = list(self.alert_collection.find(\n",
        "            {\n",
        "                'date': {'$gte': pd.to_datetime(self.start_date), '$lte': pd.to_datetime(self.end_date)},\n",
        "                'instrument': self.instrument\n",
        "            }, \n",
        "            {'_id': 0}\n",
        "        ))\n",
        "        \n",
        "        for row in data_dict:\n",
        "            self._process_alert_row(row)\n",
        "            \n",
        "        return pd.DataFrame(data_dict).drop(columns=['alerts'])\n",
        "\n",
        "    def _process_alert_row(self, row):\n",
        "        \"\"\"Process individual alert data row\"\"\"\n",
        "        # Initialize default values\n",
        "        row.update({\n",
        "            'momentum_alert': np.nan,\n",
        "            'velocity_alert': np.nan, \n",
        "            'touch_type': np.nan,\n",
        "            'count': 0.0\n",
        "        })\n",
        "\n",
        "        if not isinstance(row.get('alerts'), dict):\n",
        "            return\n",
        "\n",
        "        alerts = row['alerts']\n",
        "\n",
        "        # Process momentum and velocity alerts\n",
        "        self._process_momentum_velocity_alerts(row, alerts)\n",
        "\n",
        "        # Process EMA touch alerts\n",
        "        self._process_ema_touch_alerts(row, alerts)\n",
        "\n",
        "    def _process_momentum_velocity_alerts(self, row, alerts):\n",
        "        \"\"\"Process momentum and velocity alerts from alert data\"\"\"\n",
        "        # Process momentum alerts\n",
        "        if isinstance(alerts.get('momentum_alert'), dict):\n",
        "            row['momentum_alert'] = alerts['momentum_alert'].get('alert_type')\n",
        "        \n",
        "        # Process velocity alerts    \n",
        "        if isinstance(alerts.get('velocity_alert'), dict):\n",
        "            row['velocity_alert'] = alerts['velocity_alert'].get('alert_type')\n",
        "            \n",
        "    def _process_ema_touch_alerts(self, row, alerts):\n",
        "        \"\"\"Process EMA touch alerts from alert data\"\"\"\n",
        "        if isinstance(alerts.get('169ema_touched'), dict):\n",
        "            touch_data = alerts['169ema_touched']\n",
        "            row['touch_type'] = touch_data.get('type')\n",
        "            row['count'] = touch_data.get('count', 0.0)\n",
        "            \n",
        "        elif isinstance(alerts.get('13ema_touched'), dict):\n",
        "            touch_data = alerts['13ema_touched']\n",
        "            row['touch_type'] = touch_data.get('type')\n",
        "            row['count'] = touch_data.get('count', 0.0)\n",
        "            \n",
        "    def excute_trades(self):\n",
        "        \"\"\"Execute trades for each date in stock candidates\"\"\"\n",
        "        for idx, date in enumerate(self.stock_candidates.index.unique()):\n",
        "            self.complete_trades[date] = self.daily_capital\n",
        "            self.manage_trade(pd.to_datetime(date), idx)\n",
        "            \n",
        "    def manage_trade(self, date, idx):\n",
        "        \"\"\"Manage existing trades and look for new opportunities\"\"\"\n",
        "        if self.current_trade:\n",
        "            self._manage_existing_trade(date, idx)\n",
        "        else:\n",
        "            self._look_for_new_trade(date, idx)\n",
        "        \n",
        "    def _manage_existing_trade(self, date, idx):\n",
        "        \"\"\"Handle management of current open position\"\"\"\n",
        "        stock = self.current_trade[\"symbol\"]\n",
        "        \n",
        "        # Check if the same day as purchase\n",
        "        if self.current_trade[\"entry_date\"] == date:\n",
        "            self._log(f\"Skipping sell check - same day as purchase for {stock}\")\n",
        "            return\n",
        "        \n",
        "        # Get the current alert data for the stock (Should be the next day after the purchase)\n",
        "        tracked_data = self._get_tracked_data(stock, date)\n",
        "        if tracked_data['alert'].empty:\n",
        "            self._log(f\"No alert data found for {stock} on {date}\")\n",
        "            return\n",
        "        \n",
        "        # Apply trading rules if there is an alert\n",
        "        self._apply_trading_rules(tracked_data, idx)\n",
        "        \n",
        "    def _get_tracked_data(self, stock, date):\n",
        "        \"\"\"Get relevant price and alert data for a stock\"\"\"\n",
        "        # Get the next day data\n",
        "        next_day_data = self.df[(self.df['symbol'] == stock) & (self.df['date'] > date)].sort_values('date')\n",
        "        # Get the current day data\n",
        "        current_day_data = self.df[(self.df['symbol'] == stock) & (self.df['date'] == date)]\n",
        "        # Get the previous day data\n",
        "        previous_day_data = self.df[(self.df['symbol'] == stock) & (self.df['date'] < date)].sort_values('date')\n",
        "        \n",
        "        # Condition 1: Today is out of available trading days\n",
        "        if len(current_day_data) == 0:\n",
        "            return {\n",
        "                'price': previous_day_data.iloc[-1],\n",
        "                'price_next_day': previous_day_data.iloc[-1],\n",
        "                'alert': self.alert_df[(self.alert_df['symbol'] == stock) & \n",
        "                                    (self.alert_df['date'] == date) & \n",
        "                                    (self.alert_df['interval'] == 1)]\n",
        "            }\n",
        "        # Condition 2: No next day data\n",
        "        if len(next_day_data) == 0:\n",
        "            return {\n",
        "                'price': current_day_data.iloc[0],\n",
        "                'price_next_day': current_day_data.iloc[0],\n",
        "                'alert': self.alert_df[(self.alert_df['symbol'] == stock) & \n",
        "                                    (self.alert_df['date'] == date) & \n",
        "                                    (self.alert_df['interval'] == 1)]}\n",
        "        # Condition 3: Next day data exists\n",
        "        else:\n",
        "            return {\n",
        "                'price': current_day_data.iloc[0],\n",
        "                'price_next_day': next_day_data.iloc[0],\n",
        "                'alert': self.alert_df[(self.alert_df['symbol'] == stock) & \n",
        "                                (self.alert_df['date'] == date) & \n",
        "                                (self.alert_df['interval'] == 1)]}\n",
        "    \n",
        "    def _apply_trading_rules(self, tracked_data, idx):\n",
        "        \"\"\"Apply trading rules to determine if position should be closed\"\"\"\n",
        "        # Scenario 1: Check if we need to activate profit protection\n",
        "        self._log(\"Applying trading rules\")\n",
        "        if not self.dynamic_protection:\n",
        "            self._check_profit_protection(tracked_data)\n",
        "            \n",
        "        # Scenario 2: Dynamically manage the profit\n",
        "        elif self.dynamic_protection:\n",
        "            self._log(f\"Processing date: {tracked_data['price']['date']}\")\n",
        "            self._manage_dynamic_protection(tracked_data)\n",
        "            \n",
        "        # Scenario 3: Check for exit signals if neither profit protection nor dynamic protection is activated\n",
        "        if not self.protected and not self.dynamic_protection:\n",
        "            self._check_exit_signals(tracked_data, idx)\n",
        "        \n",
        "        # Track the daily return\n",
        "        if len(self.current_trade) != 0:\n",
        "            self.track_daily_capital(tracked_data)\n",
        "    \n",
        "    def track_daily_capital(self, tracked_data):\n",
        "        \"\"\"Track and record total account capital daily\"\"\"\n",
        "        # Calculate current total capital\n",
        "        current_price = tracked_data['price']['close']\n",
        "        entry_price = self.current_trade['entry_price']\n",
        "        position_size = self.current_trade['position_size']\n",
        "        # Calculate profit/loss and daily capital\n",
        "        profit_loss = (current_price - entry_price) * position_size\n",
        "        self.daily_capital = self.current_trade['initial_capital'] + profit_loss\n",
        "        \n",
        "        # Record the total capital for this day\n",
        "        current_date = tracked_data['price']['date'].strftime('%Y-%m-%d 00:00:00')\n",
        "        self.complete_trades[current_date] = self.daily_capital\n",
        "        \n",
        "        # # Debug logging\n",
        "        # self._log(f\"Date: {current_date}\")\n",
        "        # self._log(f\"Current price: {current_price}\")\n",
        "        # self._log(f\"Entry price: {entry_price}\")\n",
        "        # self._log(f\"Profit/Loss: {profit_loss}\")\n",
        "        # self._log(f\"Initial capital: {self.current_trade['initial_capital']}\")\n",
        "        # self._log(f\"Daily capital: {self.daily_capital}\")\n",
        "        \n",
        "    def _check_exit_signals(self, tracked_data, idx):\n",
        "        \"\"\"Check if exit signals are triggered\"\"\"\n",
        "        # Check for sell signals in alert data\n",
        "        alert_data = tracked_data['alert']\n",
        "        \n",
        "        for signal in self.sell_signals:\n",
        "            if signal in alert_data.values:\n",
        "                self._log(f\"Sell signal detected, closing position for {self.current_trade['symbol']} at {tracked_data['price']['date']}\")\n",
        "                self.track_profit_loss(tracked_data['price_next_day'], signal, sell=True)\n",
        "                return\n",
        "            \n",
        "    def _check_profit_protection(self, tracked_data):\n",
        "        \"\"\"Check if profit protection should be activated\"\"\"\n",
        "        tracked_profit_loss = self.track_profit_loss(tracked_data['price'], sell=False)\n",
        "        # Potential divide by zero if self.current_trade['cost'] is 0\n",
        "        if self.current_trade['cost'] == 0:\n",
        "            tracked_profit_pct = 0\n",
        "        else:\n",
        "            tracked_profit_pct = tracked_profit_loss / self.current_trade['cost']\n",
        "            \n",
        "        self._log(f\"Symbol: {self.current_trade['symbol']} | Date: {tracked_data['price']['date']} | Tracked profit loss: {tracked_profit_loss} | Tracked profit pct: {tracked_profit_pct}\")\n",
        "        if tracked_profit_pct >= 0.3:\n",
        "            self._activate_profit_protection(tracked_profit_loss, tracked_profit_pct)\n",
        "            self._log(\"Profit protection activated\")\n",
        "        else:\n",
        "            self._log(\"Profit protection not required\")\n",
        "            \n",
        "    def _activate_profit_protection(self, profit, profit_pct):\n",
        "        \"\"\"Activate profit protection mechanism\"\"\"\n",
        "        self.dynamic_protection = True\n",
        "        self.peak_profit = profit\n",
        "        self.peak_profit_pct = profit_pct\n",
        "\n",
        "    def _manage_dynamic_protection(self, tracked_data):\n",
        "        \"\"\"Manage dynamic profit protection\"\"\"\n",
        "        self._log(\"Dynamic protection activated\")\n",
        "        # Track profit loss\n",
        "        current_profit = self.track_profit_loss(tracked_data['price'], sell=False)\n",
        "        # Potential divide by zero if self.capital + self.current_trade['cost'] is 0\n",
        "        denominator = self.capital + self.current_trade['cost']\n",
        "        if denominator == 0:\n",
        "            current_profit_pct = 0\n",
        "        else:\n",
        "            current_profit_pct = (current_profit - self.capital + self.current_trade['cost']) / denominator\n",
        "            \n",
        "        self._log(f\"Current profit: {current_profit} | Current profit pct: {current_profit_pct} in date: {tracked_data['price']['date']}\")\n",
        "        \n",
        "        # Update peak profits\n",
        "        self._update_peak_profits(current_profit, current_profit_pct)\n",
        "        self._log(f\"Peak profit: {self.peak_profit} | Peak profit pct: {self.peak_profit_pct} in date: {tracked_data['price']['date']}\")\n",
        "        \n",
        "        # Check if profits have declined\n",
        "        if self._check_profit_decline(tracked_data, current_profit_pct):\n",
        "            self._log(f\"Profit decline detected, closing position in date: {tracked_data['price']['date']}\")\n",
        "            return\n",
        "        \n",
        "        # Check if velocity signals are triggered\n",
        "        if self._check_velocity_signals(tracked_data):\n",
        "            self._log(f\"Velocity signal detected, closing position in date: {tracked_data['price']['date']}\")\n",
        "            return\n",
        "        \n",
        "    def _update_peak_profits(self, current_profit, current_profit_pct):\n",
        "        \"\"\"Update peak profit values if current profits are higher\"\"\"\n",
        "        if current_profit > self.peak_profit:\n",
        "            self.peak_profit = current_profit\n",
        "            self.peak_profit_pct = current_profit_pct\n",
        "            \n",
        "    def _check_profit_decline(self, tracked_data, current_profit_pct):\n",
        "        \"\"\"Check if profits have declined significantly from peak\"\"\"\n",
        "        if self.peak_profit_pct - current_profit_pct >= 0.5:\n",
        "            self.track_profit_loss(tracked_data['price_next_day'], exit_reason='profit_protection', sell=True)\n",
        "            \n",
        "            # Reset the peak profit and profit protection\n",
        "            self.protected = False\n",
        "            self.peak_profit = 0\n",
        "            self.peak_profit_pct = 0\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def _check_velocity_signals(self, tracked_data):\n",
        "        \"\"\"Check velocity signals for additional protection\"\"\"\n",
        "        alert_data = tracked_data['alert']\n",
        "        for signal in self.sell_signals:\n",
        "            if signal in alert_data.values:\n",
        "                self.track_profit_loss(tracked_data['price_next_day'], exit_reason=signal, sell=True)\n",
        "                # Reset the peak profit and profit protection\n",
        "                self.protected = False\n",
        "                self.peak_profit = 0\n",
        "                self.peak_profit_pct = 0\n",
        "                return True\n",
        "        return False\n",
        "    \n",
        "    def _look_for_new_trade(self, date, idx):\n",
        "        \"\"\"Look for new trading opportunities\"\"\"\n",
        "        cur_stock_pick = self.stock_candidates.iloc[idx]\n",
        "        stock, alert = self.find_alert(cur_stock_pick, desired_alerts=self.buy_signals)\n",
        "        if stock != None:\n",
        "            self._log(f\"Looking for new trade for {stock} on {date}\")\n",
        "        else:\n",
        "            return\n",
        "        \n",
        "        self._open_new_position(stock, date, alert)\n",
        "        \n",
        "    def _open_new_position(self, stock, date, alert):\n",
        "        \"\"\"Open a new trading position\"\"\"\n",
        "        \n",
        "        # Step 1: Open a new position in the next candle open\n",
        "        next_day_data = self.df[(self.df['symbol'] == stock) & (self.df['date'] == date + pd.Timedelta(days=1))]\n",
        "        if next_day_data.empty:\n",
        "            return\n",
        "        avg_cost = next_day_data['open'].iloc[0]\n",
        "        \n",
        "        # Step 2: Calculate fees and available capital\n",
        "        portion = self.dynamic_portion(self.capital) if self.dynamic_asset_control else 1\n",
        "        trading_capital = self.capital * portion\n",
        "        fees = trading_capital * self.buy_fee\n",
        "        post_fees_capital = trading_capital - fees\n",
        "        \n",
        "        # Step 3: Calculate position size\n",
        "        position_size = post_fees_capital / avg_cost\n",
        "        position_size = position_size if position_size < 0 else int(position_size)\n",
        "        \n",
        "        # Step 4: Calculate actual cost\n",
        "        total_cost = avg_cost * position_size\n",
        "        \n",
        "        # Step 5: Verify we have enough capital\n",
        "        if total_cost > post_fees_capital:\n",
        "            self._log(f\"Insufficient capital for trade in {stock}\")\n",
        "            return\n",
        "        # Step 6: Caculate Remaining Capital    \n",
        "        remaining_capital = self.capital - total_cost\n",
        "        \n",
        "        self._log(f\"Next day data for {stock} on {next_day_data['date'].iloc[0]}\")\n",
        "        self._log(f\"Position size: {position_size:.8f} shares at ${avg_cost:.2f}\")\n",
        "        self._log(f\"Cost: ${total_cost:.8f}\")\n",
        "        self._log(f\"Fees: ${fees:.8f}\")\n",
        "        self._log(f\"Remaining capital: ${remaining_capital:.8f}\")\n",
        "        self.current_trade = {\n",
        "            \"initial_capital\": post_fees_capital + remaining_capital,\n",
        "            \"entry_date\": next_day_data['date'].iloc[0],\n",
        "            \"symbol\": stock,\n",
        "            \"entry_price\": avg_cost,\n",
        "            \"position_size\": position_size,\n",
        "            \"fees\": fees,\n",
        "            \"cost\": total_cost,\n",
        "            \"remaining_capital\": remaining_capital,\n",
        "            \"entry_reason\": alert\n",
        "        }\n",
        "        \n",
        "        # Deduct cost from available capital\n",
        "        self.capital = post_fees_capital - total_cost\n",
        "        \n",
        "        self._log(f\"Opened new position for {stock} on {next_day_data['date'].iloc[0]}\")\n",
        "        self._log(f\"Remaining capital: ${self.capital:.2f}\")\n",
        "    \n",
        "    def dynamic_portion(self, available_capital):\n",
        "        \"\"\"Calculate the dynamic portion of the available capital\"\"\"\n",
        "        # All in if available capital is less than 10000\n",
        "        if available_capital <= 10000:\n",
        "            return 1\n",
        "        # 50% if available capital is less than 100000\n",
        "        elif (available_capital <= 100000) and (available_capital > 10000):\n",
        "            return 0.5\n",
        "        # 25% if available capital is less than 1000000\n",
        "        elif (available_capital <= 1000000) and (available_capital >= 100000):\n",
        "            return 0.25\n",
        "        # 10% if available capital is less than 10000000\n",
        "        else:\n",
        "            return 0.1\n",
        "        \n",
        "    def track_profit_loss(self, tracked_data, exit_reason=None, sell=False):\n",
        "        \"\"\"Track and calculate profit/loss for a position\"\"\"\n",
        "        \n",
        "        # Track current trade\n",
        "        exit_price = tracked_data['close'] \n",
        "        entry_price = self.current_trade['entry_price']\n",
        "        cost = self.current_trade['cost']\n",
        "        profit_loss = (exit_price - entry_price) * self.current_trade['position_size']\n",
        "                \n",
        "        # Potential divide by zero if cost is 0\n",
        "        if cost == 0:\n",
        "            profit_rate = 0\n",
        "        else:\n",
        "            profit_rate = profit_loss / cost\n",
        "        \n",
        "        # Close the position if sell signal is detected\n",
        "        if sell:\n",
        "            # Close the position\n",
        "            self._close_position(tracked_data, exit_price, profit_loss, exit_reason)\n",
        "            return None\n",
        "        \n",
        "        return profit_loss\n",
        "    \n",
        "    def _close_position(self, tracked_data, exit_price, profit_loss, exit_reason):\n",
        "        \"\"\"Close out an existing position and record the trade\"\"\"\n",
        "        \n",
        "        # Step 1. Get the amount of capital after sold\n",
        "        gain_loss_after_sold = profit_loss + self.current_trade[\"cost\"] # Reamining captial + profit or loss + the total cost\n",
        "        \n",
        "        # Step 2. Compute the actual final captial after fees\n",
        "        self.capital = (self.current_trade['remaining_capital'] + gain_loss_after_sold) - (gain_loss_after_sold * self.sell_fee)\n",
        "        \n",
        "        # Step 3. Compute the profit rate\n",
        "        # Potential divide by zero if self.current_trade[\"cost\"] is 0\n",
        "        if self.current_trade[\"cost\"] == 0:\n",
        "            profit_rate = 0\n",
        "        else:\n",
        "            profit_rate = profit_loss / self.current_trade[\"cost\"]\n",
        "        \n",
        "        self.trades.append({\n",
        "            \"symbol\": self.current_trade[\"symbol\"],\n",
        "            \"entry_price\": self.current_trade[\"entry_price\"],\n",
        "            \"entry_date\": self.current_trade[\"entry_date\"],\n",
        "            \"exit_price\": exit_price,\n",
        "            \"exit_date\": tracked_data['date'],\n",
        "            \"position_size\": self.current_trade[\"position_size\"],\n",
        "            \"cost\": f\"{self.current_trade['cost']:.2f}\",\n",
        "            \"profit/loss%\": f\"{profit_rate * 100:.2f}%\",\n",
        "            \"profit/loss\": f\"{profit_loss:.2f}\",\n",
        "            \"remaining_capital\": f\"{self.current_trade['remaining_capital']:.2f}\",\n",
        "            \"total_capital\": f\"{self.capital:.2f}\",\n",
        "            \"entry_reason\": self.current_trade[\"entry_reason\"],\n",
        "            \"exit_reason\": exit_reason,\n",
        "            \"holding_period_days\": (tracked_data['date'] - self.current_trade[\"entry_date\"]).days\n",
        "            \n",
        "        })\n",
        "        \n",
        "        # Reset trade state\n",
        "        self.current_trade = {}\n",
        "        \n",
        "    def find_alert(self, stock_data, desired_alerts):\n",
        "        \"\"\"Find matching alerts in stock data\"\"\"\n",
        "        for alert in desired_alerts:\n",
        "            cur_alert_data = stock_data.loc[alert]\n",
        "            if len(cur_alert_data) != 0:\n",
        "                return stock_data[alert][0], alert\n",
        "        return None, None\n",
        "    \n",
        "    def run(self):\n",
        "        \"\"\"Run the trading strategy\"\"\"\n",
        "        self.excute_trades()\n",
        "        trade_df = self.get_trades()\n",
        "        captial_df = self.get_daily_return()\n",
        "        return trade_df, captial_df \n",
        "    \n",
        "    def get_daily_return(self): \n",
        "        \"\"\"Get DataFrame of daily returns\"\"\"\n",
        "        complete_trades_df = pd.DataFrame(self.complete_trades.items(), columns=['date', 'long_capital'])\n",
        "        return complete_trades_df\n",
        "    \n",
        "    def get_trades(self):\n",
        "        \"\"\"Get DataFrame of completed trades\"\"\"\n",
        "        return pd.DataFrame(self.trades)\n",
        "    \n",
        "    def _log(self, message):\n",
        "        \"\"\"Log debug messages if verbose mode is enabled\"\"\"\n",
        "        if self.verbose:\n",
        "            print(message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "strategy = LongTermTradingStrategy(mongo_config=mongo_config, \n",
        "                                start_date='2020-01-01',\n",
        "                                end_date='2024-12-31',\n",
        "                                sandbox_mode=True,\n",
        "                                instrument='crypto',\n",
        "                                verbose=False,\n",
        "                                buy_signals=['long_accelerating'],\n",
        "                                sell_signals=['velocity_loss'])\n",
        "\n",
        "trade_df, captial_df = strategy.run()\n",
        "captial_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trade_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complete Trade History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CompleteTradeHistory:\n",
        "    def __init__(self, mongo_config, instrument):\n",
        "        self.mongo_client = MongoClient(mongo_config['connection_string'])\n",
        "        self.mongo_db = self.mongo_client[mongo_config['db']]\n",
        "        self.raw_data_collection = f\"{mongo_config['warehouse_interval']}_data\"\n",
        "        self.raw_data = pd.DataFrame(list(self.mongo_db[self.raw_data_collection].find({'instrument': instrument})))\n",
        "\n",
        "    def _position_live_update(self, trade_data, holding_symbol_data, all_trade_data):\n",
        "        live_update_data = []\n",
        "        for holding_timestamp in holding_symbol_data['date']:\n",
        "            # Close out an existing position and record the trade\n",
        "            # Step 1. Current Profit Loss Computation\n",
        "            \n",
        "            buy_price = trade_data['entry_price']\n",
        "            current_price = holding_symbol_data[holding_symbol_data['date'] == holding_timestamp]['close'].iloc[0]\n",
        "            cur_profit_loss = (current_price - buy_price) * trade_data['position_size']\n",
        "            \n",
        "            # Step 2. Current Profit Loss Pct Computation\n",
        "            cur_profit_loss_pct = cur_profit_loss / float(trade_data['cost'])\n",
        "\n",
        "            # Step 3. Compute the current capital given the current profit loss\n",
        "            current_capital = (float(trade_data['remaining_capital']) + cur_profit_loss) + float(trade_data['cost'])    \n",
        "            \n",
        "            live_update_data.append({\n",
        "                \"symbol\": trade_data[\"symbol\"],\n",
        "                \"entry_price\": trade_data[\"entry_price\"], \n",
        "                \"entry_date\": trade_data[\"entry_date\"],\n",
        "                \"exit_price\": current_price,\n",
        "                \"exit_date\": holding_timestamp,\n",
        "                \"position_size\": trade_data[\"position_size\"],\n",
        "                \"cost\": trade_data[\"cost\"],\n",
        "                \"profit_loss_pct\": f\"{cur_profit_loss_pct * 100}%\",\n",
        "                \"profit/loss\": f\"{cur_profit_loss}\",\n",
        "                \"remaining_capital\": trade_data[\"remaining_capital\"],\n",
        "                \"total_capital\": f\"{current_capital:.2f}\",\n",
        "                \"holding_period_days\": (holding_timestamp - trade_data[\"entry_date\"]).days\n",
        "            })\n",
        "            \n",
        "        live_update_data = pd.DataFrame(live_update_data)\n",
        "        \n",
        "        # Find all matching trades for this symbol and entry date\n",
        "        matching_trades = all_trade_data[\n",
        "            (all_trade_data['symbol'] == trade_data['symbol']) & \n",
        "            (all_trade_data['entry_date'] == trade_data['entry_date'])\n",
        "        ].index\n",
        "        \n",
        "        if len(matching_trades) > 0:\n",
        "            # Get the first matching trade index\n",
        "            trade_index_before = matching_trades[0]\n",
        "            trade_index_after = trade_index_before + 1\n",
        "            \n",
        "            # Concat the dataframes\n",
        "            all_trade_data = pd.concat(\n",
        "                [all_trade_data.iloc[:trade_index_before], live_update_data, all_trade_data.iloc[trade_index_after:]], \n",
        "                ignore_index=True)\n",
        "        else:\n",
        "            # If no matching trade found, append to end\n",
        "            all_trade_data = pd.concat([all_trade_data, live_update_data], ignore_index=True)\n",
        "            \n",
        "        return all_trade_data\n",
        "\n",
        "    def generate_complete_history(self, results_df):\n",
        "        all_trade_data = results_df.copy()\n",
        "        \n",
        "        for index, trade_data in results_df.iterrows():\n",
        "            start_trade_date = trade_data['entry_date']\n",
        "            end_trade_date = trade_data['exit_date']\n",
        "            trade_symbol = trade_data['symbol']\n",
        "            holding_symbol_data = self.raw_data[\n",
        "                (self.raw_data['symbol'] == trade_symbol) & \n",
        "                (self.raw_data['date'] > start_trade_date) & \n",
        "                (self.raw_data['date'] <= end_trade_date)\n",
        "            ]\n",
        "            all_trade_data = self._position_live_update(trade_data, holding_symbol_data, all_trade_data)\n",
        "            \n",
        "        return all_trade_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "complete_trade_history = CompleteTradeHistory(mongo_config, instrument='crypto').generate_complete_history(trade_df)\n",
        "complete_trade_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Back Test Evaluation\n",
        "\n",
        "A class designed to backtest and analyze the performance of all combinations of buy and sell signals\n",
        "\n",
        "Key Features:\n",
        "- Loads historical stock data, alerts, and stock candidates from MongoDB\n",
        "- Implements a dynamic profit protection mechanism that triggers when profits reach 30%\n",
        "- Tracks and analyzes stock performance based on velocity alerts\n",
        "- Records trade results including entry/exit dates, profits/losses, and exit reasons\n",
        "\n",
        "Testing Logic:\n",
        "1. **Entry**: Test out all the signals in the buy_signals list\n",
        "2. **Exit Conditions**: Test out all the signals in the sell_signals list\n",
        "\n",
        "Usage:\n",
        "- Study the dual tunnels strategy performance assuming client uses the system at any point in a year\n",
        "- Use the results to estimate the potential returns and risks of the dual tunnels strategy\n",
        "- Study the reason of exit and entry to improve the strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class StrategyEDA:\n",
        "    def __init__(self, mongo_config, start_date, end_date, buy_signals, sell_signals, instrument):\n",
        "        self.mongo_config = mongo_config\n",
        "        self.start_date = pd.to_datetime(start_date)\n",
        "        self.end_date = pd.to_datetime(end_date)\n",
        "        self.instrument = instrument\n",
        "        self.title = self.process_title(buy_signals, sell_signals)\n",
        "    \n",
        "    def process_title(self, buy_signals, sell_signals):\n",
        "        buy_signals = ', '.join(buy_signals)\n",
        "        sell_signals = ', '.join(sell_signals)\n",
        "        return f'{self.instrument} Buy: {buy_signals} | Sell: {sell_signals}'\n",
        "    \n",
        "    def get_nasdaq_return_data(self):\n",
        "        mongo_client = MongoClient(self.mongo_config['connection_string'])\n",
        "        nasdaq_data = pd.DataFrame(list(mongo_client[self.mongo_config['db']][self.mongo_config['warehouse_interval'] + '_data'].\\\n",
        "            find({'symbol': '^IXIC',\n",
        "                    'date': {'$gte': self.start_date, '$lte': self.end_date},\n",
        "                    },\n",
        "                    {'date': 1, 'close': 1, '_id': 0})))\n",
        "        nasdaq_data['return'] = nasdaq_data['close'].pct_change()\n",
        "        nasdaq_data.dropna(inplace=True)\n",
        "        \n",
        "        # Assuming 10000 initial capital\n",
        "        nasdaq_data['total_capital'] = 10000 * (1 + nasdaq_data['return']).cumprod()\n",
        "        nasdaq_data.set_index('date', inplace=True)\n",
        "        return nasdaq_data\n",
        "    \n",
        "    def get_bitcoin_return_data(self):\n",
        "        mongo_client = MongoClient(self.mongo_config['connection_string'])\n",
        "        bitcoin_data = pd.DataFrame(list(mongo_client[self.mongo_config['db']][self.mongo_config['warehouse_interval'] + '_data'].\\\n",
        "            find({'symbol': 'BTC',\n",
        "                    'date': {'$gte': self.start_date, '$lte': self.end_date},\n",
        "                    },\n",
        "                    {'date': 1, 'close': 1, '_id': 0})))\n",
        "        bitcoin_data['return'] = bitcoin_data['close'].pct_change()\n",
        "        bitcoin_data.dropna(inplace=True)\n",
        "        \n",
        "        # Assuming 10000 initial capital\n",
        "        bitcoin_data['total_capital'] = 10000 * (1 + bitcoin_data['return']).cumprod()    \n",
        "        bitcoin_data.set_index('date', inplace=True)\n",
        "        return bitcoin_data\n",
        "    \n",
        "    def plot_trading_analysis(self, results_df):\n",
        "        import plotly.graph_objects as go\n",
        "        from plotly.subplots import make_subplots\n",
        "        \n",
        "        # Convert profit/loss from string to float\n",
        "        results_df['profit_loss%'] = results_df['profit/loss%'].str.rstrip('%').astype(float)\n",
        "        \n",
        "        # Create subplots with different heights\n",
        "        fig = make_subplots(rows=2, cols=1, \n",
        "                        row_heights=[0.7, 0.3],\n",
        "                        subplot_titles=(f'Portfolio Value Over Time',\n",
        "                                        f'Distribution of Returns'))\n",
        "        \n",
        "        # Process data for portfolio value plot\n",
        "        results_df_sorted = results_df.sort_values('exit_date')\n",
        "        first_data = pd.DataFrame({'total_capital': [10000], 'exit_date': [pd.to_datetime(self.start_date)]})\n",
        "        pd.set_option(\"future.no_silent_downcasting\", True)\n",
        "        results_df_sorted = pd.concat([first_data, results_df_sorted]).fillna(0)\n",
        "        results_df_sorted = results_df_sorted.reset_index(drop=True)\n",
        "        results_df_sorted.set_index('exit_date', inplace=True)\n",
        "\n",
        "        # Create date range and interpolate\n",
        "        date_range = pd.date_range(start=pd.to_datetime(self.start_date), \n",
        "                                end=pd.to_datetime(self.end_date), \n",
        "                                freq='D')\n",
        "        results_df_sorted = results_df_sorted.reindex(date_range)\n",
        "        results_df_sorted['total_capital'] = results_df_sorted['total_capital'].astype(float)\n",
        "        results_df_sorted['total_capital'] = results_df_sorted['total_capital'].interpolate()\n",
        "        results_df_sorted['monthly_return'] = results_df_sorted['total_capital'].pct_change(periods=30)\n",
        "        \n",
        "        # Add Comparison Data\n",
        "        if self.instrument == 'equity':\n",
        "            nasdaq_data = self.get_nasdaq_return_data()\n",
        "            nasdaq_data = nasdaq_data[self.start_date:self.end_date]\n",
        "        elif self.instrument == 'crypto':\n",
        "            bitcoin_data = self.get_bitcoin_return_data()\n",
        "            bitcoin_data = bitcoin_data[self.start_date:self.end_date]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid instrument: {self.instrument}\")\n",
        "        \n",
        "        # Plot strategy line\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=results_df_sorted.index, \n",
        "                    y=results_df_sorted['total_capital'],\n",
        "                    name='Strategy',\n",
        "                    line=dict(color='blue')),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # Plot Bitcoin\n",
        "        if self.instrument == 'crypto':\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=bitcoin_data.index, \n",
        "                        y=bitcoin_data['total_capital'],\n",
        "                    name='Bitcoin',\n",
        "                    line=dict(color='rgba(247, 147, 26, 0.5)')),  # Bitcoin orange\n",
        "            row=1, col=1\n",
        "            )\n",
        "        # Plot NASDAQ\n",
        "        if self.instrument == 'equity':\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=nasdaq_data.index, \n",
        "                    y=nasdaq_data['total_capital'],\n",
        "                    name='NASDAQ',\n",
        "                    line=dict(color='rgba(85, 85, 85, 0.5)')),  # Dark gray\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # Add initial principle line\n",
        "        fig.add_hline(y=10000, line_dash=\"dash\", line_color=\"gray\", \n",
        "                    annotation_text=\"Initial Principle\",\n",
        "                    row=1, col=1)\n",
        "    \n",
        "        # Add bear market shading\n",
        "        fig.add_vrect(\n",
        "            x0=\"2022-01-01\", x1=\"2022-12-31\",\n",
        "            fillcolor=\"red\", opacity=0.1,\n",
        "            layer=\"below\", line_width=0,\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # Add final values annotation\n",
        "        strategy_final = results_df_sorted['total_capital'].iloc[-1]\n",
        "        if self.instrument == 'equity':\n",
        "            nasdaq_final = nasdaq_data['total_capital'].iloc[-1]\n",
        "        elif self.instrument == 'crypto':\n",
        "            bitcoin_final = bitcoin_data['total_capital'].iloc[-1]\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid instrument: {self.instrument}\")\n",
        "        \n",
        "        # Position annotations based on log scale\n",
        "        max_y = results_df_sorted['total_capital'].max()\n",
        "        \n",
        "        if self.instrument == 'equity':\n",
        "            fig.add_annotation(\n",
        "                text=f'Strategy Final: ${strategy_final:,.2f} (NASDAQ Final: ${nasdaq_final:,.2f})',\n",
        "                x=pd.to_datetime(self.start_date) + pd.Timedelta(days=365),\n",
        "                y=max_y * 0.9,\n",
        "                showarrow=False,\n",
        "                row=1, col=1\n",
        "            )\n",
        "        elif self.instrument == 'crypto':\n",
        "            fig.add_annotation(\n",
        "                text=f'Strategy Final: ${strategy_final:,.2f} (Bitcoin Final: ${bitcoin_final:,.2f})',\n",
        "                x=pd.to_datetime(self.start_date) + pd.Timedelta(days=365),\n",
        "                y=max_y * 0.9,\n",
        "                showarrow=False,\n",
        "                row=1, col=1\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid instrument: {self.instrument}\")\n",
        "        # Add monthly return annotations adjusted for log scale\n",
        "        for i in range(0, len(results_df_sorted), 30):\n",
        "            if i+30 < len(results_df_sorted):\n",
        "                monthly_return = results_df_sorted['monthly_return'].iloc[i+ 30]\n",
        "                if not pd.isna(monthly_return):\n",
        "                    date = results_df_sorted.index[i+ 30]\n",
        "                    current_value = results_df_sorted['total_capital'].iloc[i+ 30]\n",
        "                    y_pos = current_value * 1.1  # Position 10% above the current value \n",
        "                    color = 'green' if monthly_return > 0 else 'red' \n",
        "                    fig.add_annotation( \n",
        "                        text=f'{monthly_return:.3%}', \n",
        "                        x=date, \n",
        "                        y=y_pos, \n",
        "                        showarrow=False, \n",
        "                        font=dict(color=color, size=10), \n",
        "                        textangle=90, \n",
        "                        row=1,  \n",
        "                        col=1\n",
        "                    )\n",
        "        \n",
        "        # Add returns distribution plot\n",
        "        fig.add_trace(\n",
        "            go.Histogram(x=results_df['profit_loss%'],\n",
        "                        name='Returns Distribution',\n",
        "                        nbinsx=180),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # Add vertical line at x=0 for distribution\n",
        "        fig.add_vline(x=0, line_dash=\"dash\", line_color=\"red\", row=2, col=1)\n",
        "\n",
        "        # Update layout with log y-axis\n",
        "        fig.update_layout(\n",
        "            height=800,\n",
        "            width=1200,\n",
        "            showlegend=True,\n",
        "            title_text=self.title,\n",
        "            xaxis=dict(\n",
        "                range=[pd.to_datetime(self.start_date), pd.to_datetime(self.end_date)]\n",
        "            ),\n",
        "            yaxis=dict(type='linear')  # Set y-axis to logarithmic scale\n",
        "        )\n",
        "        \n",
        "        # Update axes labels\n",
        "        fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
        "        fig.update_xaxes(title_text=\"Return %\", row=2, col=1,)\n",
        "        fig.update_yaxes(title_text=\"Total Amount ($)\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"Count\", row=2, col=1)\n",
        "        \n",
        "        # Show the plot\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StrategyAnalytics:\n",
        "    def __init__(self, verbose=True, instrument='crypto', trade_type='bull'):\n",
        "        self.verbose = verbose\n",
        "        self.instrument = instrument\n",
        "        self.trade_type = trade_type\n",
        "        \n",
        "    def _base_case_analysis(self, bull_result_df, crypto_df):\n",
        "        base_case_dict = {}\n",
        "        # Get the base case\n",
        "        symbol_list = bull_result_df['symbol'].unique()\n",
        "        for symbol in symbol_list:\n",
        "            symbol_df = crypto_df.xs(symbol, axis=0)\n",
        "            # Compute return rate \n",
        "            return_rate = symbol_df.iloc[-1]['close'] / symbol_df.iloc[0]['close']\n",
        "            # Compute Max Drawdown\n",
        "            max_drawdown = symbol_df['close'].div(symbol_df['close'].cummax()).sub(1).min()\n",
        "            base_case_dict[symbol] = {'return_rate': return_rate, 'max_drawdown': max_drawdown}\n",
        "        base_case_df = pd.DataFrame(base_case_dict)\n",
        "        return base_case_df\n",
        "    \n",
        "    def monthly_return_rate(self, capital_df):\n",
        "        # Get the monthly return rate\n",
        "        capital_col = 'long_capital' if self.trade_type == 'bull' else 'short_capital'\n",
        "        df = capital_df[['date', capital_col]]\n",
        "        X = df.copy()\n",
        "        # Group by month and get last value, keeping the date column\n",
        "        X = X.groupby(['date']).last().reset_index()\n",
        "        X['monthly_return_rate'] = X[capital_col].pct_change()\n",
        "        \n",
        "        return X\n",
        "    \n",
        "    def sharpe_ratio(self, captial_df):\n",
        "        # Create a copy of results_df for manipulation\n",
        "        X = captial_df.copy()\n",
        "        X['date'] = pd.to_datetime(X['date'])\n",
        "        capital_col = 'long_capital' if self.trade_type == 'bull' else 'short_capital'\n",
        "        X['daily_return'] = X[capital_col].pct_change()\n",
        "        \n",
        "        # Handle any NaN values in daily returns\n",
        "        X['daily_return'] = X['daily_return'].fillna(0)\n",
        "        \n",
        "        unique_year = X['date'].dt.year.nunique()\n",
        "        \n",
        "        # Last day capital\n",
        "        last_day_capital = X[capital_col].iloc[-1]\n",
        "        \n",
        "        # Initial capital\n",
        "        initial_capital = X[capital_col].iloc[0]\n",
        "        \n",
        "        # Risk free rate\n",
        "        risk_free_rate = 0.03\n",
        "        \n",
        "        # Excessive daily return \n",
        "        mean_annualized_return = (last_day_capital/ initial_capital) ** ((1/unique_year)) - 1\n",
        "        \n",
        "        # Calculate standard deviation avoiding NaN/invalid values\n",
        "        daily_returns = X['daily_return'].replace([np.inf, -np.inf], np.nan).dropna()\n",
        "        std_annualized_return = daily_returns.std() * np.sqrt(365)\n",
        "        \n",
        "        # Avoid division by zero\n",
        "        if std_annualized_return == 0:\n",
        "            return 0\n",
        "            \n",
        "        # Compute annualized Sharpe Ratio\n",
        "        sharpe_ratio = ((mean_annualized_return - risk_free_rate) / std_annualized_return)\n",
        "        \n",
        "        return sharpe_ratio\n",
        "\n",
        "    def max_drawdown(self, captial_df):\n",
        "        \"\"\"Calculate the maximum drawdown percentage from peak to trough, considering chronological order.\n",
        "        \n",
        "        Args:\n",
        "            captial_df: DataFrame containing capital column with daily portfolio values\n",
        "            \n",
        "        Returns:\n",
        "            float: Maximum drawdown percentage (0 to 100)\n",
        "        \"\"\"\n",
        "        X = captial_df.copy()\n",
        "        X['date'] = pd.to_datetime(X['date'])\n",
        "        X = X.sort_values('date')  # Ensures chronological order\n",
        "        \n",
        "        capital_col = 'long_capital' if self.trade_type == 'bull' else 'short_capital'\n",
        "        \n",
        "        # This is key - expanding().max() only considers values up to current point\n",
        "        running_max = X[capital_col].expanding().max()\n",
        "        \n",
        "        # Calculate drawdown in percentage terms\n",
        "        drawdown = ((X[capital_col] - running_max) / running_max) * -100\n",
        "        \n",
        "        # Get the maximum drawdown\n",
        "        max_dd = drawdown.max()\n",
        "        \n",
        "        return max_dd\n",
        "    \n",
        "    @staticmethod\n",
        "    def win_rate(results_df):\n",
        "        # Create a copy of results_df for manipulation\n",
        "        X = results_df.copy()\n",
        "\n",
        "        X['profit/loss%'] = X['profit/loss%'].str.rstrip('%').astype(float)\n",
        "        win_trade = X[X['profit/loss%'] > 0]\n",
        "        loss_trade = X[X['profit/loss%'] < 0]\n",
        "        \n",
        "        # Handle division by zero case\n",
        "        total_trades = len(win_trade) + len(loss_trade)\n",
        "        if total_trades == 0:\n",
        "            return 0\n",
        "            \n",
        "        return len(win_trade) / total_trades\n",
        "    \n",
        "    @staticmethod\n",
        "    def max_profit(results_df):\n",
        "        # Create a copy of results_df for manipulation\n",
        "        X = results_df.copy()\n",
        "        X['profit/loss%'] = X['profit/loss%'].str.rstrip('%').astype(float)\n",
        "        return X['profit/loss%'].max()\n",
        "\n",
        "    @staticmethod\n",
        "    def max_loss(results_df):\n",
        "        X = results_df.copy()\n",
        "        X['profit/loss%'] = X['profit/loss%'].str.rstrip('%').astype(float)\n",
        "        return X['profit/loss%'].min()\n",
        "\n",
        "    @staticmethod\n",
        "    def median_return(results_df):\n",
        "        # Create a copy of results_df for manipulation\n",
        "        X = results_df.copy()\n",
        "        X['profit/loss%'] = X['profit/loss%'].str.rstrip('%').astype(float)\n",
        "        return X['profit/loss%'].median()\n",
        "        \n",
        "    @staticmethod\n",
        "    def mean_return(results_df):\n",
        "        # Create a copy of results_df for manipulation\n",
        "        X = results_df.copy()\n",
        "        X['profit/loss%'] = X['profit/loss%'].str.rstrip('%').astype(float)\n",
        "        return X['profit/loss%'].mean()\n",
        "        \n",
        "    @staticmethod\n",
        "    def std_return(results_df):\n",
        "        # Create a copy of results_df for manipulation\n",
        "        X = results_df.copy()\n",
        "        X['profit/loss%'] = X['profit/loss%'].str.rstrip('%').astype(float)\n",
        "        return X['profit/loss%'].std()\n",
        "    \n",
        "    @staticmethod\n",
        "    def final_capital(results_df):\n",
        "        X = results_df.copy()\n",
        "        X['total_capital'] = X['total_capital'].astype(float)\n",
        "        return X['total_capital'].iloc[-1]\n",
        "\n",
        "    def annualized_return(self, results_df):\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "\n",
        "        # Create a copy of results_df for manipulation\n",
        "        X = results_df.copy()\n",
        "\n",
        "        # Strip '%' and convert profit/loss% to numeric\n",
        "        X['profit/loss%'] = X['profit/loss%'].str.rstrip('%').astype(float)\n",
        "\n",
        "        # Set Exit_date as the index for easier grouping\n",
        "        X['exit_date'] = pd.to_datetime(X['exit_date'])\n",
        "        X.set_index('exit_date', inplace=True)\n",
        "\n",
        "        # Group by year and calculate annualized returns\n",
        "        annualized_returns = []\n",
        "        for year, group in X.groupby(X.index.year):\n",
        "            # Calculate cumulative return for the year\n",
        "            cumulative_return = np.prod(1 + (group['profit/loss%'] / 100)) - 1\n",
        "\n",
        "            # Calculate the number of days in the year\n",
        "            days_in_year = group.index.dayofyear.max()\n",
        "            \n",
        "            # Handle division by zero case\n",
        "            if days_in_year == 0:\n",
        "                continue\n",
        "\n",
        "            # Annualize the return\n",
        "            if self.instrument == 'equity':\n",
        "                annualized = (1 + cumulative_return) ** (252 / days_in_year) - 1\n",
        "            else:  # Assume 365 days for non-equity instruments\n",
        "                annualized = (1 + cumulative_return) ** (365 / days_in_year) - 1\n",
        "\n",
        "            # Store the result as a list\n",
        "            annualized_returns.append([year, annualized])\n",
        "\n",
        "        # Convert to DataFrame with column names\n",
        "        annualized_returns = pd.DataFrame(annualized_returns, columns=['year', 'annualized_return'])\n",
        "\n",
        "        return annualized_returns\n",
        "\n",
        "    def run(self, results_df, captial_df):\n",
        "        # Create a copy for manipulation\n",
        "        X = results_df.copy()\n",
        "        Y = captial_df.copy()\n",
        "        # Calculate all metrics once\n",
        "        metrics = {\n",
        "            'sharpe_ratio': self.sharpe_ratio(Y),\n",
        "            'max_drawdown': self.max_drawdown(Y),\n",
        "            'final_capital': self.final_capital(X),\n",
        "            'median_return': self.median_return(X), \n",
        "            'mean_return': self.mean_return(X),\n",
        "            'std_return': self.std_return(X),\n",
        "            'max_profit': self.max_profit(X),\n",
        "            'max_loss': self.max_loss(X),\n",
        "            'annualized_return': self.annualized_return(X)\n",
        "        }\n",
        "        \n",
        "        # Print summary using calculated metrics\n",
        "        if self.verbose:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"TRADING STRATEGY PERFORMANCE SUMMARY\") \n",
        "            print(\"=\"*50 + \"\\n\")\n",
        "            \n",
        "            print(\"YEARLY RETURNS\")\n",
        "            print(\"-\"*20)\n",
        "            print(metrics['annualized_return'])\n",
        "\n",
        "            print(\"\\n\")\n",
        "            \n",
        "            print(\"KEY STATISTICS\")\n",
        "            print(\"-\"*20)\n",
        "            print(f\"Final Capital: ${metrics['final_capital']:,.2f}\")\n",
        "            print(f\"Sharpe Ratio: {metrics['sharpe_ratio']:.2f}\")\n",
        "            print(f\"Maximum Drawdown: {metrics['max_drawdown']:.2f}%\")\n",
        "            print(f\"Maximum Profit: {metrics['max_profit']:.2f}%\")\n",
        "            print(f\"Maximum Loss: {metrics['max_loss']:.2f}%\")\n",
        "            print(\"\\n\")\n",
        "            \n",
        "            print(\"RETURN METRICS\")\n",
        "            print(\"-\"*20)\n",
        "            print(f\"Mean Return: {metrics['mean_return']:.2f}%\")\n",
        "            print(f\"Median Return: {metrics['median_return']:.2f}%\")\n",
        "            print(f\"Return Std Dev: {metrics['std_return']:.2f}%\")\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            \n",
        "        return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Equity Trading Backtest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import combinations, product\n",
        "\n",
        "output_dir = os.path.join('..', 'data', 'strategies_combination_results')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# Stock Candidates Generation\n",
        "instrument = 'equity'\n",
        "pick_stock_instance = Pick_Stock(mongo_config, instrument=instrument, sandbox_mode=True, start_date='2020-01-01')\n",
        "stock_candidates_df = pick_stock_instance.run()\n",
        "\n",
        "# Define all available buy and sell signals\n",
        "buy_signals = ['accumulating', 'long_accumulating', 'accelerating', 'long_accelerating', 'velocity_maintained']\n",
        "sell_signals = ['velocity_loss', 'velocity_weak']\n",
        "\n",
        "# Generate all combinations of buy and sell signals\n",
        "max_signals = len(buy_signals)  # Maximum signals to include in a combination\n",
        "all_buy_combinations = [\n",
        "    list(comb) for r in range(1, max_signals + 1) for comb in combinations(buy_signals, r)\n",
        "]\n",
        "all_sell_combinations = [\n",
        "    list(comb) for r in range(1, len(sell_signals) + 1) for comb in combinations(sell_signals, r)\n",
        "]\n",
        "# Test all combinations\n",
        "strategy_results = {}\n",
        "for buy_combination, sell_combination in product(all_buy_combinations, all_sell_combinations):\n",
        "    # Run the trading strategy\n",
        "    trading_strategy = LongTermTradingStrategy(\n",
        "        mongo_config=mongo_config,\n",
        "        start_date='2020-01-01',\n",
        "        end_date='2024-12-31',\n",
        "        sandbox_mode=True,\n",
        "        instrument=instrument,\n",
        "        buy_signals=buy_combination,\n",
        "        sell_signals=sell_combination,\n",
        "        verbose=False\n",
        "    )\n",
        "    print(f'Running {buy_combination} | {sell_combination}')\n",
        "    results_df, captial_df = trading_strategy.run()\n",
        "    # Only keep the results that has more than 50,000 final capital and total assest never drops below 10,000\n",
        "    results_df['total_capital'] = results_df['total_capital'].astype(float)\n",
        "    if results_df['total_capital'].iloc[-1] > 50000:\n",
        "        # Save results to CSV\n",
        "        results_df.to_csv(os.path.join(output_dir, f'results_{instrument}_{buy_combination}_{sell_combination}.csv'), index=False)\n",
        "        # Run the analytics\n",
        "        strategy_analytics = StrategyAnalytics(instrument=instrument)\n",
        "        analytics_results = strategy_analytics.run(results_df, captial_df)\n",
        "        # Run the EDA analysis\n",
        "        eda_analysis = StrategyEDA(mongo_config=mongo_config, start_date='2020-01-01', end_date='2024-12-31', buy_signals=buy_combination, sell_signals=sell_combination, instrument=instrument)\n",
        "        eda_analysis.plot_trading_analysis(results_df)\n",
        "        # Store results\n",
        "        combination_key = f\"Buy: {buy_combination} | Sell: {sell_combination}\"\n",
        "        strategy_results[combination_key] = analytics_results\n",
        "    else:\n",
        "        continue\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "buy = ['long_accumulating','accelerating','velocity_maintained']\n",
        "sell = ['velocity_weak']    \n",
        "trading_strategy = LongTermTradingStrategy(\n",
        "    mongo_config=mongo_config,\n",
        "    start_date='2020-01-01',\n",
        "    end_date='2024-12-31',\n",
        "    sandbox_mode=True,\n",
        "    instrument=instrument,\n",
        "    buy_signals=buy,\n",
        "    sell_signals=sell,\n",
        "    verbose=False\n",
        ")\n",
        "results_df, captial_df = trading_strategy.run()\n",
        "eda_analysis = StrategyEDA(mongo_config=mongo_config, start_date='2020-01-01', end_date='2024-12-31', buy_signals=buy, sell_signals=sell, instrument=instrument)\n",
        "eda_analysis.plot_trading_analysis(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the best combinations based on multiple metrics\n",
        "metrics = {\n",
        "    'sharpe_ratio': {'best': float('-inf'), 'combination': None},\n",
        "    'final_capital': {'best': float('-inf'), 'combination': None}, \n",
        "    'max_drawdown': {'best': float('inf'), 'combination': None}  # For drawdown, lower is better\n",
        "}\n",
        "\n",
        "for combination, results in strategy_results.items():\n",
        "    # Check each metric\n",
        "    if results['sharpe_ratio'] > metrics['sharpe_ratio']['best']:\n",
        "        metrics['sharpe_ratio']['best'] = results['sharpe_ratio']\n",
        "        metrics['sharpe_ratio']['combination'] = combination\n",
        "    if results['final_capital'] > metrics['final_capital']['best']:\n",
        "        metrics['final_capital']['best'] = results['final_capital']\n",
        "        metrics['final_capital']['combination'] = combination\n",
        "    if results['max_drawdown'] < metrics['max_drawdown']['best']:\n",
        "        metrics['max_drawdown']['best'] = results['max_drawdown']\n",
        "        metrics['max_drawdown']['combination'] = combination\n",
        "        \n",
        "# Print results for each metric\n",
        "print(\"Best combinations by metric:\")\n",
        "print(f\"\\nBest by Sharpe Ratio ({metrics['sharpe_ratio']['best']:.4f}):\")\n",
        "print(f\"Combination: {metrics['sharpe_ratio']['combination']}\")\n",
        "\n",
        "print(f\"\\nBest by Final Capital (${metrics['final_capital']['best']:,.2f}):\")\n",
        "print(f\"Combination: {metrics['final_capital']['combination']}\")\n",
        "\n",
        "print(f\"\\nBest by Max Drawdown ({metrics['max_drawdown']['best']:.2%}):\")\n",
        "print(f\"Combination: {metrics['max_drawdown']['combination']}\")\n",
        "\n",
        "print(\"\\nCombinations with Sharpe ratio > 1:\")\n",
        "for combination, results in strategy_results.items():\n",
        "    if results['sharpe_ratio'] > 1:\n",
        "        print(f\"\\nCombination: {combination}\")\n",
        "        print(f\"Sharpe Ratio: {results['sharpe_ratio']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Crypto Trading Backtest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import combinations, product\n",
        "mongo_config = load_client_config('mongodb', 'production')\n",
        "url = mongo_config['connection_string']\n",
        "mongo_client = MongoClient(url)\n",
        "all_sandbox_collections = mongo_client['data_lake']['sandbox_results']\n",
        "\n",
        "output_dir = os.path.join('..', 'data', 'strategies_combination_results')\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# Stock Candidates Generation\n",
        "instrument = 'crypto'\n",
        "pick_stock_instance = Pick_Stock(mongo_config, instrument=instrument, sandbox_mode=True, start_date='2020-01-01')\n",
        "stock_candidates_df = pick_stock_instance.run()\n",
        "\n",
        "# Define all available buy and sell signals\n",
        "buy_signals = ['accumulating', 'long_accumulating', 'accelerating', 'long_accelerating', 'velocity_maintained']\n",
        "sell_signals = ['velocity_loss', 'velocity_weak']\n",
        "\n",
        "# Generate all combinations of buy and sell signals\n",
        "max_signals = len(buy_signals)  # Maximum signals to include in a combination\n",
        "all_buy_combinations = [\n",
        "    list(comb) for r in range(1, max_signals + 1) for comb in combinations(buy_signals, r)\n",
        "]\n",
        "all_sell_combinations = [\n",
        "    list(comb) for r in range(1, len(sell_signals) + 1) for comb in combinations(sell_signals, r)\n",
        "]\n",
        "\n",
        "# Test all combinations\n",
        "crypto_strategy_results = {}\n",
        "for buy_combination, sell_combination in product(all_buy_combinations, all_sell_combinations):\n",
        "    # Run the trading strategy\n",
        "    trading_strategy = LongTermTradingStrategy(\n",
        "        mongo_config=mongo_config,\n",
        "        start_date='2020-01-01',\n",
        "        end_date='2024-12-31',\n",
        "        sandbox_mode=True,\n",
        "        instrument=instrument,\n",
        "        buy_signals=buy_combination,\n",
        "        sell_signals=sell_combination,\n",
        "        verbose=False\n",
        "    )\n",
        "    \n",
        "    results_df, captial_df = trading_strategy.run()\n",
        "    results_df['total_capital'] = results_df['total_capital'].astype(float)\n",
        "    # Run the analytics\n",
        "    strategy_analytics = StrategyAnalytics(instrument=instrument)\n",
        "    analytics_results = strategy_analytics.run(results_df.drop(columns=['exit_reason', 'entry_reason']), captial_df)\n",
        "    \n",
        "    # Only keep the results that has more than 300,000 final capital and total assest never drops below 10,000\n",
        "    if (results_df['total_capital'].iloc[-1] > 300000) and (analytics_results['max_drawdown'] < 50) and (analytics_results['sharpe_ratio'] > 1):\n",
        "        \n",
        "        results_df['buy_signals'] = f\"{buy_combination}\"\n",
        "        results_df['sell_signals'] = f\"{sell_combination}\"\n",
        "        \n",
        "        captial_df['buy_signals'] = f\"{buy_combination}\"\n",
        "        captial_df['sell_signals'] = f\"{sell_combination}\"\n",
        "        \n",
        "        # Save results to MongoDB\n",
        "        mongo_client['data_lake']['sandbox_trade_history'].insert_many(results_df.to_dict('records'))  \n",
        "        mongo_client['data_lake']['sandbox_daily_capital'].insert_many(captial_df.to_dict('records'))   \n",
        "        # Run the EDA analysis\n",
        "        eda_analysis = StrategyEDA(mongo_config=mongo_config, start_date='2020-01-01', end_date='2024-12-31', buy_signals=buy_combination, sell_signals=sell_combination, instrument=instrument)\n",
        "        eda_analysis.plot_trading_analysis(results_df)\n",
        "        # Store results\n",
        "        combination_key = f\"Buy: {buy_combination} | Sell: {sell_combination}\"\n",
        "        crypto_strategy_results[combination_key] = analytics_results\n",
        "    else:\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the best combinations based on multiple metrics\n",
        "crypto_metrics = {\n",
        "    'sharpe_ratio': {'best': float('-inf'), 'combination': None},\n",
        "    'final_capital': {'best': float('-inf'), 'combination': None}, \n",
        "    'max_drawdown': {'best': float('inf'), 'combination': None}  # For drawdown, lower is better\n",
        "}\n",
        "\n",
        "for combination, results in crypto_strategy_results.items():\n",
        "    # Check each metric\n",
        "    if results['sharpe_ratio'] > crypto_metrics['sharpe_ratio']['best']:\n",
        "        crypto_metrics['sharpe_ratio']['best'] = results['sharpe_ratio']\n",
        "        crypto_metrics['sharpe_ratio']['combination'] = combination\n",
        "    if results['final_capital'] > crypto_metrics['final_capital']['best']:\n",
        "        crypto_metrics['final_capital']['best'] = results['final_capital']\n",
        "        crypto_metrics['final_capital']['combination'] = combination\n",
        "    if results['max_drawdown'] < crypto_metrics['max_drawdown']['best']:\n",
        "        crypto_metrics['max_drawdown']['best'] = results['max_drawdown']\n",
        "        crypto_metrics['max_drawdown']['combination'] = combination\n",
        "        \n",
        "# Print results for each metric\n",
        "print(\"Best combinations by metric:\")\n",
        "print(f\"\\nBest by Sharpe Ratio ({crypto_metrics['sharpe_ratio']['best']:.4f}):\")\n",
        "print(f\"Combination: {crypto_metrics['sharpe_ratio']['combination']}\")\n",
        "\n",
        "print(f\"\\nBest by Final Capital (${crypto_metrics['final_capital']['best']:,.2f}):\")\n",
        "print(f\"Combination: {crypto_metrics['final_capital']['combination']}\")\n",
        "\n",
        "print(f\"\\nBest by Max Drawdown ({crypto_metrics['max_drawdown']['best']:.2%}):\")\n",
        "print(f\"Combination: {crypto_metrics['max_drawdown']['combination']}\") \n",
        "\n",
        "# Print combinations with Sharpe ratio > 1, sorted by Sharpe ratio\n",
        "print(\"\\nCombinations with Sharpe ratio > 1 (sorted by Sharpe ratio):\")\n",
        "sorted_results = sorted(\n",
        "    [(combination, results) for combination, results in crypto_strategy_results.items() if results['sharpe_ratio'] > 1],\n",
        "    key=lambda x: x[1]['sharpe_ratio'],\n",
        "    reverse=True\n",
        ")\n",
        "for combination, results in sorted_results:\n",
        "    print(f\"\\nCombination: {combination}\")\n",
        "    print(f\"Sharpe Ratio: {results['sharpe_ratio']:.4f} | Final Capital: ${results['final_capital']:,.2f} | Max Drawdown: {results['max_drawdown']:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monte Carlo Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the MongoDB configuration\n",
        "mongo_config = load_client_config('mongodb', 'production')\n",
        "\n",
        "# Load the trading backtest results\n",
        "client = MongoClient(mongo_config['url'])\n",
        "db_name = mongo_config['db_name']\n",
        "collection = client[db_name]['sandbox_results']\n",
        "\n",
        "back_test_results = pd.DataFrame(list(collection.find({'buy_signals': \"['long_accelerating']\",\n",
        "                                                        'sell_signals': \"['velocity_loss']\"})))\n",
        "\n",
        "gain_loss_pct = back_test_results['profit/loss%'].str.rstrip('%').astype(float) / 100\n",
        "\n",
        "trade_history = gain_loss_pct.to_numpy()\n",
        "print(trade_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Randomize trade sequences\n",
        "num_simulations = 1000\n",
        "num_trades = len(trade_history)\n",
        "\n",
        "simulated_equity_curves = []\n",
        "for _ in range(num_simulations):\n",
        "    randomized_trades = np.random.choice(trade_history, size=num_trades, replace=True)\n",
        "    equity_curve = np.cumsum(randomized_trades)  # Cumulative sum for equity curve\n",
        "    simulated_equity_curves.append(equity_curve)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add Gaussian noise to trades\n",
        "noise_std_dev = 0.5  # Adjust based on expected variability\n",
        "simulated_equity_curves_with_noise = []\n",
        "for _ in range(num_simulations):\n",
        "    noisy_trades = trade_history + np.random.normal(0, noise_std_dev, num_trades)\n",
        "    equity_curve = np.cumsum(noisy_trades)\n",
        "    simulated_equity_curves_with_noise.append(equity_curve)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate extreme market conditions by amplifying losses and gains\n",
        "extreme_factor = 1.5  # Amplify outcomes by a factor\n",
        "extreme_trades = trade_history * extreme_factor\n",
        "\n",
        "simulated_extreme_curves = []\n",
        "for _ in range(num_simulations):\n",
        "    extreme_randomized_trades = np.random.choice(extreme_trades, size=num_trades, replace=True)\n",
        "    equity_curve = np.cumsum(extreme_randomized_trades)\n",
        "    simulated_extreme_curves.append(equity_curve)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert results to DataFrame for analysis\n",
        "simulated_equity_curves_df = pd.DataFrame(simulated_equity_curves)\n",
        "simulated_equity_curves_with_noise_df = pd.DataFrame(simulated_equity_curves_with_noise)\n",
        "simulated_extreme_curves_df = pd.DataFrame(simulated_extreme_curves)\n",
        "\n",
        "# Calculate final equity values and drawdowns\n",
        "def analyze_results(simulated_df):\n",
        "    final_equity_values = simulated_df.iloc[:, -1]\n",
        "    max_drawdowns = simulated_df.apply(lambda x: np.maximum.accumulate(x) - x, axis=1).max(axis=1)\n",
        "\n",
        "    # Summary statistics\n",
        "    avg_final_equity = final_equity_values.mean()\n",
        "    worst_drawdown = max_drawdowns.max()\n",
        "    confidence_interval = np.percentile(final_equity_values, [5, 95])\n",
        "\n",
        "    return avg_final_equity, worst_drawdown, confidence_interval\n",
        "\n",
        "# Analyze each scenario\n",
        "results_randomized = analyze_results(simulated_equity_curves_df)\n",
        "results_with_noise = analyze_results(simulated_equity_curves_with_noise_df)\n",
        "results_extreme = analyze_results(simulated_extreme_curves_df)\n",
        "\n",
        "print(\"Randomized Trades:\", results_randomized)\n",
        "print(\"With Noise:\", results_with_noise)\n",
        "print(\"Extreme Conditions:\", results_extreme)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot sample equity curves for one scenario (randomized trades)\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(len(simulated_equity_curves_df)):  # Plot first 10 simulations\n",
        "    plt.plot(simulated_equity_curves_df.iloc[i], alpha=0.5)\n",
        "plt.title(\"Sample Monte Carlo Simulated Equity Curves (Randomized Trades)\")\n",
        "plt.xlabel(\"Trade Number\")\n",
        "plt.ylabel(\"Equity\")\n",
        "plt.show()\n",
        "\n",
        "# Plot histogram of final equity values for all scenarios\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(simulated_equity_curves_df.iloc[:, -1], bins=30, alpha=0.7, label=\"Randomized\")\n",
        "plt.hist(simulated_equity_curves_with_noise_df.iloc[:, -1], bins=30, alpha=0.7, label=\"With Noise\")\n",
        "plt.hist(simulated_extreme_curves_df.iloc[:, -1], bins=30, alpha=0.7, label=\"Extreme\")\n",
        "plt.title(\"Distribution of Final Equity Values\")\n",
        "plt.xlabel(\"Final Equity\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Macro Economic Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pymongo import DESCENDING\n",
        "import pandas as pd\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "import yfinance as yf\n",
        "\n",
        "class MacroEconomicAnalysis:\n",
        "    def __init__(self, mongo_url='localhost', db_name='data_lake', collection_name='long_term_alert'):\n",
        "        self.mongo_url = mongo_url\n",
        "        self.db_name = db_name\n",
        "        self.collection_name = collection_name\n",
        "        self.current_date = pd.Timestamp('2024-11-22')\n",
        "        self.time_periods = {\n",
        "            'last_2_weeks': pd.Timedelta(days=14),\n",
        "            'last_month': pd.Timedelta(days=30), \n",
        "            'last_3_months': pd.Timedelta(days=90)\n",
        "        }\n",
        "        \n",
        "    def load_data(self):\n",
        "        # Create MongoDB client connection once\n",
        "        mongo_client = MongoClient(self.mongo_url)\n",
        "        collection = mongo_client[self.db_name][self.collection_name]\n",
        "        \n",
        "        results = {}\n",
        "        \n",
        "        # Get distinct symbols for index, commodity and bond instruments\n",
        "        target_instruments = ['index', 'commodity', 'bond', 'sector']\n",
        "        for instrument in target_instruments:\n",
        "            symbols = collection.distinct('symbol', {'instrument': instrument})\n",
        "            results[instrument] = {}\n",
        "            \n",
        "            for symbol in symbols:\n",
        "                # Get all historical data for each symbol and time period\n",
        "                results[instrument][symbol] = {}\n",
        "                for period_name, period_delta in self.time_periods.items():\n",
        "                    start_date = self.current_date - period_delta\n",
        "                    \n",
        "                    results[instrument][symbol][period_name] = list(collection.find(\n",
        "                        {\n",
        "                            'symbol': symbol,\n",
        "                            'interval': 1,\n",
        "                            'date': {'$gte': start_date}\n",
        "                        },\n",
        "                        {'close': 1, '_id': 0}\n",
        "                    ).sort('date', DESCENDING))\n",
        "                \n",
        "        return results\n",
        "    \n",
        "    def compute_correlations(self, data):\n",
        "        correlations = {}\n",
        "        \n",
        "        for period in self.time_periods.keys():\n",
        "            # Create empty dataframe for each time period\n",
        "            period_df = pd.DataFrame()\n",
        "            \n",
        "            # Add data for each instrument and symbol\n",
        "            for instrument in data:\n",
        "                for symbol in data[instrument]:\n",
        "                    symbol_data = pd.DataFrame(data[instrument][symbol][period])\n",
        "                    if not symbol_data.empty:\n",
        "                        # Calculate returns using pct_change\n",
        "                        \n",
        "                        returns = pd.Series(symbol_data['close']).pct_change()\n",
        "                        period_df[f\"{symbol}\"] = returns\n",
        "            \n",
        "            # Calculate correlation matrix on returns for this period\n",
        "            correlations[period] = period_df.corr(method='pearson')\n",
        "        \n",
        "        return correlations\n",
        "    \n",
        "    def run(self):\n",
        "        # Load data for all instruments and symbols\n",
        "        data = self.load_data()\n",
        "        # Compute correlations between all symbols for each time period\n",
        "        correlations = self.compute_correlations(data)\n",
        "        \n",
        "        return correlations\n",
        "    \n",
        "macro_economic_analysis = MacroEconomicAnalysis()\n",
        "correlations = macro_economic_analysis.run()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fundamentals Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from polygon import RESTClient\n",
        "import yfinance as yf \n",
        "# Ploygon API\n",
        "api_key = load_client_config(tool='websocket', mode='production')['api_key']\n",
        "client = RESTClient(api_key)\n",
        "\n",
        "# Fetch Related Companies\n",
        "ticker = \"AAPL\"\n",
        "\n",
        "try:\n",
        "    related_companies = client.get_related_companies(ticker)\n",
        "    tickers = [company.ticker for company in related_companies]\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching related companies: {e}\")\n",
        "\n",
        "\n",
        "# Fetch Fundamentals\n",
        "for ticker in tickers:\n",
        "    try:    \n",
        "        yf_ticker = yf.Ticker(ticker)\n",
        "        current_price = yf_ticker.history(period='1d')['Close'].iloc[0]\n",
        "        eps = yf_ticker.info['trailingEps']\n",
        "        price_to_eps = current_price / eps\n",
        "        print(f\"{ticker}: {eps} | {price_to_eps}\")\n",
        "    except Exception as e:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas_market_calendars as mcal\n",
        "import pytz\n",
        "from datetime import datetime\n",
        "\n",
        "# Get NYSE calendar\n",
        "nyse = mcal.get_calendar('NYSE')\n",
        "\n",
        "# Get current NY time\n",
        "ny_tz = pytz.timezone('America/New_York')\n",
        "today = datetime.now(ny_tz).strftime('%Y-%m-%d')\n",
        "\n",
        "# Get market schedule\n",
        "schedule = nyse.schedule(start_date=today, end_date=today)\n",
        "\n",
        "# Get market hours\n",
        "market_open = schedule.iloc[0]['market_open'].tz_convert('America/New_York')\n",
        "market_close = schedule.iloc[0]['market_close'].tz_convert('America/New_York')\n",
        "\n",
        "print(market_open)\n",
        "print(market_close)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".cloud_dp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
